{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import *\n",
    "from keras.layers import concatenate, CuDNNGRU\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        #self.init = initializations.get('glorot_uniform')\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # eij = K.dot(x, self.W) TF backend doesn't support it\n",
    "\n",
    "        # features_dim = self.W.shape[0]\n",
    "        # step_dim = x._keras_shape[1]\n",
    "\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "    #print weigthted_input.shape\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        #return input_shape[0], input_shape[-1]\n",
    "        return input_shape[0],  self.features_dim\n",
    "        \n",
    "# path = '..'\n",
    "# EMBEDDING_FILE=path+'/input/glove.840B.300d.txt'\n",
    "# EMBEDDING_FILE=path+'/input/crawl-300d-2M.vec'\n",
    "# TRAIN_DATA_FILE=path+'/input/train.csv'\n",
    "# TEST_DATA_FILE=path+'/input/test.csv'\n",
    "\n",
    "EMBEDDING_FILE='/public/models/fasttext/crawl-300d-2M.vec'\n",
    "# EMBEDDING_FILE='/public/models/glove/glove.840B.300d.txt'\n",
    "TRAIN_DATA_FILE='/public/toxic_comments/train.csv'\n",
    "TEST_DATA_FILE='/public/toxic_comments/test.csv'\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 150\n",
    "MAX_NB_WORDS = 100000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "num_lstm = 300\n",
    "num_dense = 256\n",
    "rate_drop_lstm = 0.2\n",
    "rate_drop_dense = 0.2\n",
    "\n",
    "act = 'relu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanData(text, stemming = False, lemmatize=False):    \n",
    "    text = text.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+\\-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    if stemming:\n",
    "        st = PorterStemmer()\n",
    "        txt = \" \".join([st.stem(w) for w in text.split()])\n",
    "    if lemmatize:\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        txt = \" \".join([wordnet_lemmatizer.lemmatize(w) for w in text.split()])\n",
    "    return text\n",
    "\n",
    "\n",
    "cols_f = ['count_sent', 'count_word', 'count_unique_word', 'count_letters', 'count_punctuations', \n",
    "          'count_stopwords', 'mean_word_len', 'word_unique_percent', 'punct_percent', 'num_exclamation_marks'\n",
    "          , 'num_question_marks', 'you_count']\n",
    "cols_mm = ['count_sent', 'count_word', 'count_unique_word', 'count_letters', 'count_punctuations', \n",
    "           'count_stopwords', 'mean_word_len', 'word_unique_percent', 'punct_percent', 'num_exclamation_marks'\n",
    "           ,'num_question_marks', 'you_count']\n",
    "\n",
    "def get_features(df):\n",
    "    df['count_sent']=df[\"comment_text\"].apply(lambda x: len(re.findall(\"\\n\",str(x)))+1)\n",
    "    df['count_word']=df[\"comment_text\"].apply(lambda x: len(str(x).split()))\n",
    "    df['count_unique_word']=df[\"comment_text\"].apply(lambda x: len(set(str(x).split())))\n",
    "    df['count_letters']=df[\"comment_text\"].apply(lambda x: len(str(x)))\n",
    "    df[\"count_punctuations\"] =df[\"comment_text\"].apply(lambda x: len([c for c in str(x) if c in \n",
    "                                                                      string.punctuation]))\n",
    "    df[\"count_stopwords\"] = df[\"comment_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stopwords]))\n",
    "    df[\"mean_word_len\"] = df[\"comment_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "    df['word_unique_percent']=df['count_unique_word']*100/df['count_word']\n",
    "    df['punct_percent']=df['count_punctuations']*100/df['count_word']\n",
    "    df['num_exclamation_marks'] = df['comment_text'].apply(lambda comment: comment.count('!'))\n",
    "    df['num_question_marks'] = df['comment_text'].apply(lambda comment: comment.count('?'))\n",
    "    df['you_count'] = df['comment_text'].apply(lambda comment: sum(comment.count(w) for w in ('you', 'You', 'YOU')))\n",
    "    scaler = MinMaxScaler().fit(df[cols_mm])\n",
    "    df[cols_mm] = scaler.transform(df[cols_mm])\n",
    "    return df\n",
    "\n",
    "train_df = get_features(train)\n",
    "test_df = get_features(test)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors\n",
      "Found 2195895 word vectors of glove.\n",
      "-0.01444638 0.47249147\n",
      "Total 2195895 word vectors.\n",
      "Processing text dataset\n",
      "Found 292462 unique tokens\n",
      "Shape of data tensor: (159571, 100)\n",
      "Shape of label tensor: (159571, 6)\n",
      "Shape of test_data tensor: (153164, 100)\n"
     ]
    }
   ],
   "source": [
    "print('Indexing word vectors')\n",
    "\n",
    "count = 0\n",
    "embeddings_index = {}\n",
    "f = open(EMBEDDING_FILE)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = ' '.join(values[:-300])\n",
    "    coefs = np.asarray(values[-300:], dtype='float32')\n",
    "    embeddings_index[word] = coefs.reshape(-1)\n",
    "    coef = embeddings_index[word]\n",
    "f.close()\n",
    "\n",
    "print('Found %d word vectors of glove.' % len(embeddings_index))\n",
    "emb_mean,emb_std = coef.mean(), coef.std()\n",
    "print(emb_mean,emb_std)\n",
    "\n",
    "print('Total %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test_df = pd.read_csv(TEST_DATA_FILE)\n",
    "\n",
    "print('Processing text dataset')\n",
    "\n",
    "train_df['comment_text'] = train_df['comment_text'].map(lambda x: cleanData(x,  stemming = False, \n",
    "                                                                            lemmatize=False))\n",
    "test_df['comment_text'] = test_df['comment_text'].map(lambda x: cleanData(x,  stemming = False, \n",
    "                                                                          lemmatize=False))\n",
    "\n",
    "#Regex to remove all Non-Alpha Numeric and space\n",
    "special_character_removal=re.compile(r'[^a-z\\d ]',re.IGNORECASE)\n",
    "#regex to replace all numerics\n",
    "replace_numbers=re.compile(r'\\d+',re.IGNORECASE)\n",
    "\n",
    "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    #Remove Special Characters\n",
    "    text=special_character_removal.sub('',text)\n",
    "    #Replace Numbers\n",
    "    text=replace_numbers.sub('n',text)\n",
    "\n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    return(text)\n",
    "\n",
    "\n",
    "list_sentences_train = train_df[\"comment_text\"].fillna(\"NA\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train_df[list_classes].values\n",
    "list_sentences_test = test_df[\"comment_text\"].fillna(\"NA\").values\n",
    "\n",
    "\n",
    "comments = []\n",
    "for text in list_sentences_train:\n",
    "    comments.append(text_to_wordlist(text))\n",
    "    \n",
    "test_comments=[]\n",
    "for text in list_sentences_test:\n",
    "    test_comments.append(text_to_wordlist(text))\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(comments + test_comments)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(comments)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_comments)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', y.shape)\n",
    "\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of test_data tensor:', test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_post = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH,padding='post', truncating='post')\n",
    "print('Shape of data tensor:', data_post.shape)\n",
    "print('Shape of label tensor:', y.shape)\n",
    "\n",
    "test_data_post = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "print('Shape of test_data tensor:', test_data_post.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: 21603\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix')\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "adam_opt = Adam(lr=1e-3, decay=0.001)\n",
    "\n",
    "def get_model():\n",
    "    comment_input = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "    comment_input_post = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "\n",
    "    x1 = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix], \n",
    "                  input_length=MAX_SEQUENCE_LENGTH)(comment_input)\n",
    "    x1 = SpatialDropout1D(0.4)(x1)\n",
    "    x1 = Bidirectional(CuDNNGRU(80, return_sequences=True))(x1)\n",
    "    avg_pool1 = GlobalAveragePooling1D()(x1)\n",
    "    max_pool1 = GlobalMaxPooling1D()(x1)\n",
    "    \n",
    "    x2 = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix], \n",
    "                  input_length=MAX_SEQUENCE_LENGTH)(comment_input_post)\n",
    "    x2 = SpatialDropout1D(0.4)(x2)\n",
    "    x2 = Bidirectional(CuDNNGRU(80, return_sequences=True))(x2)\n",
    "    avg_pool2 = GlobalAveragePooling1D()(x2)\n",
    "    max_pool2 = GlobalMaxPooling1D()(x2)\n",
    "\n",
    "    conc = concatenate([avg_pool1, max_pool1, avg_pool2, max_pool2])\n",
    "    conc = Dropout(0.2)(conc)\n",
    "    preds = Dense(6, activation=\"sigmoid\")(conc)\n",
    "    model = Model(inputs=[comment_input, comment_input_post], outputs=preds)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam_opt, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "import numpy as np\n",
    "\n",
    "test_predicts_list = []\n",
    "\n",
    "def train_folds(data, data_post, y, fold_count, batch_size):\n",
    "    print(\"Starting to train models...\")\n",
    "    fold_size = len(data) // fold_count\n",
    "    models = []\n",
    "    for fold_id in range(0, fold_count):\n",
    "        fold_start = fold_size * fold_id\n",
    "        fold_end = fold_start + fold_size\n",
    "\n",
    "        if fold_id == fold_size - 1:\n",
    "            fold_end = len(data)\n",
    "\n",
    "        print(\"Fold {0}\".format(fold_id))\n",
    "        \n",
    "        train_x = np.concatenate([data[:fold_start], data[fold_end:]])\n",
    "        train_xp = np.concatenate([data_post[:fold_start], data_post[fold_end:]])\n",
    "        train_y = np.concatenate([y[:fold_start], y[fold_end:]])\n",
    "\n",
    "        val_x = data[fold_start:fold_end]\n",
    "        val_xp = data_post[fold_start:fold_end]\n",
    "        val_y = y[fold_start:fold_end]\n",
    "        \n",
    "        file_path=\"attngru_pp_fold{0}.h5\".format(fold_id)\n",
    "        model = get_model()\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=1)\n",
    "        callbacks_list = [checkpoint, early] \n",
    "\n",
    "        hist = model.fit([train_x, train_xp], train_y, epochs=10, batch_size=256, shuffle=True, \n",
    "                         validation_data=([val_x, val_xp], val_y), callbacks = callbacks_list, verbose=1)\n",
    "        model.load_weights(file_path)\n",
    "        best_score = min(hist.history['val_loss'])\n",
    "        \n",
    "        print(\"Fold {0} loss {1}\".format(fold_id, best_score))\n",
    "        print(\"Predicting results...\")\n",
    "        test_predicts_path = \"attngru_pp_test_predicts{0}.npy\".format(fold_id)\n",
    "        test_predicts = model.predict([test_data, test_data_post], batch_size=1024, verbose=1)\n",
    "        test_predicts_list.append(test_predicts)\n",
    "        np.save(test_predicts_path, test_predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train models...\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0707 - acc: 0.9766Epoch 00000: val_loss improved from inf to 0.04589, saving model to attngru_fold0.h5\n",
      "143614/143614 [==============================] - 366s - loss: 0.0707 - acc: 0.9766 - val_loss: 0.0459 - val_acc: 0.9824\n",
      "Epoch 2/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0453 - acc: 0.9830Epoch 00001: val_loss improved from 0.04589 to 0.04380, saving model to attngru_fold0.h5\n",
      "143614/143614 [==============================] - 358s - loss: 0.0453 - acc: 0.9830 - val_loss: 0.0438 - val_acc: 0.9828\n",
      "Epoch 3/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0397 - acc: 0.9845Epoch 00002: val_loss improved from 0.04380 to 0.04163, saving model to attngru_fold0.h5\n",
      "143614/143614 [==============================] - 357s - loss: 0.0397 - acc: 0.9845 - val_loss: 0.0416 - val_acc: 0.9839\n",
      "Epoch 4/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9860Epoch 00003: val_loss did not improve\n",
      "143614/143614 [==============================] - 359s - loss: 0.0351 - acc: 0.9860 - val_loss: 0.0438 - val_acc: 0.9833\n",
      "Epoch 5/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9873Epoch 00004: val_loss did not improve\n",
      "143614/143614 [==============================] - 359s - loss: 0.0313 - acc: 0.9873 - val_loss: 0.0466 - val_acc: 0.9831\n",
      "Fold 0 loss 0.0416301521112232\n",
      "Predicting results...\n",
      "153164/153164 [==============================] - 32s    \n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0685 - acc: 0.9778Epoch 00000: val_loss improved from inf to 0.04706, saving model to attngru_fold1.h5\n",
      "143614/143614 [==============================] - 354s - loss: 0.0684 - acc: 0.9778 - val_loss: 0.0471 - val_acc: 0.9816\n",
      "Epoch 2/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9830Epoch 00001: val_loss improved from 0.04706 to 0.04377, saving model to attngru_fold1.h5\n",
      "143614/143614 [==============================] - 350s - loss: 0.0448 - acc: 0.9830 - val_loss: 0.0438 - val_acc: 0.9827\n",
      "Epoch 3/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9845Epoch 00002: val_loss did not improve\n",
      "143614/143614 [==============================] - 348s - loss: 0.0394 - acc: 0.9845 - val_loss: 0.0438 - val_acc: 0.9832\n",
      "Epoch 4/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9859Epoch 00003: val_loss did not improve\n",
      "143614/143614 [==============================] - 348s - loss: 0.0352 - acc: 0.9859 - val_loss: 0.0449 - val_acc: 0.9822\n",
      "Fold 1 loss 0.04376676583336605\n",
      "Predicting results...\n",
      "153164/153164 [==============================] - 32s    \n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0732 - acc: 0.9759Epoch 00000: val_loss improved from inf to 0.04788, saving model to attngru_fold2.h5\n",
      "143614/143614 [==============================] - 363s - loss: 0.0732 - acc: 0.9759 - val_loss: 0.0479 - val_acc: 0.9818\n",
      "Epoch 2/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9828Epoch 00001: val_loss improved from 0.04788 to 0.04375, saving model to attngru_fold2.h5\n",
      "143614/143614 [==============================] - 363s - loss: 0.0455 - acc: 0.9828 - val_loss: 0.0438 - val_acc: 0.9832\n",
      "Epoch 3/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9842Epoch 00002: val_loss improved from 0.04375 to 0.04300, saving model to attngru_fold2.h5\n",
      "143614/143614 [==============================] - 358s - loss: 0.0400 - acc: 0.9842 - val_loss: 0.0430 - val_acc: 0.9839\n",
      "Epoch 4/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9858Epoch 00003: val_loss did not improve\n",
      "143614/143614 [==============================] - 357s - loss: 0.0354 - acc: 0.9858 - val_loss: 0.0445 - val_acc: 0.9837\n",
      "Epoch 5/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9871Epoch 00004: val_loss did not improve\n",
      "143614/143614 [==============================] - 357s - loss: 0.0317 - acc: 0.9871 - val_loss: 0.0473 - val_acc: 0.9833\n",
      "Fold 2 loss 0.04299884445056204\n",
      "Predicting results...\n",
      "153164/153164 [==============================] - 32s    \n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0708 - acc: 0.9763Epoch 00000: val_loss improved from inf to 0.04741, saving model to attngru_fold3.h5\n",
      "143614/143614 [==============================] - 361s - loss: 0.0708 - acc: 0.9763 - val_loss: 0.0474 - val_acc: 0.9824\n",
      "Epoch 2/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9831Epoch 00001: val_loss improved from 0.04741 to 0.04554, saving model to attngru_fold3.h5\n",
      "143614/143614 [==============================] - 359s - loss: 0.0450 - acc: 0.9831 - val_loss: 0.0455 - val_acc: 0.9832\n",
      "Epoch 3/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0397 - acc: 0.9845Epoch 00002: val_loss improved from 0.04554 to 0.04549, saving model to attngru_fold3.h5\n",
      "143614/143614 [==============================] - 359s - loss: 0.0397 - acc: 0.9845 - val_loss: 0.0455 - val_acc: 0.9833\n",
      "Epoch 4/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9874Epoch 00004: val_loss did not improve\n",
      "143614/143614 [==============================] - 353s - loss: 0.0312 - acc: 0.9874 - val_loss: 0.0515 - val_acc: 0.9828\n",
      "Fold 3 loss 0.045493070253055407\n",
      "Predicting results...\n",
      "153164/153164 [==============================] - 32s    \n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0731 - acc: 0.9753Epoch 00000: val_loss improved from inf to 0.04504, saving model to attngru_fold4.h5\n",
      "143614/143614 [==============================] - 360s - loss: 0.0731 - acc: 0.9753 - val_loss: 0.0450 - val_acc: 0.9831\n",
      "Epoch 2/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9828Epoch 00001: val_loss improved from 0.04504 to 0.04354, saving model to attngru_fold4.h5\n",
      "143614/143614 [==============================] - 356s - loss: 0.0456 - acc: 0.9828 - val_loss: 0.0435 - val_acc: 0.9828\n",
      "Epoch 3/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9844Epoch 00002: val_loss improved from 0.04354 to 0.04150, saving model to attngru_fold4.h5\n",
      "143614/143614 [==============================] - 356s - loss: 0.0398 - acc: 0.9844 - val_loss: 0.0415 - val_acc: 0.9844\n",
      "Epoch 4/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9858Epoch 00003: val_loss did not improve\n",
      "143614/143614 [==============================] - 356s - loss: 0.0352 - acc: 0.9858 - val_loss: 0.0451 - val_acc: 0.9839\n",
      "Epoch 5/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9873Epoch 00004: val_loss did not improve\n",
      "143614/143614 [==============================] - 352s - loss: 0.0316 - acc: 0.9873 - val_loss: 0.0476 - val_acc: 0.9835\n",
      "Fold 4 loss 0.04150328691704437\n",
      "Predicting results...\n",
      "153164/153164 [==============================] - 33s    \n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0741 - acc: 0.9745Epoch 00000: val_loss improved from inf to 0.04468, saving model to attngru_fold5.h5\n",
      "143614/143614 [==============================] - 359s - loss: 0.0741 - acc: 0.9745 - val_loss: 0.0447 - val_acc: 0.9832\n",
      "Epoch 2/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9826Epoch 00001: val_loss improved from 0.04468 to 0.04312, saving model to attngru_fold5.h5\n",
      "143614/143614 [==============================] - 357s - loss: 0.0463 - acc: 0.9826 - val_loss: 0.0431 - val_acc: 0.9841\n",
      "Epoch 3/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9843Epoch 00002: val_loss improved from 0.04312 to 0.04283, saving model to attngru_fold5.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143614/143614 [==============================] - 355s - loss: 0.0403 - acc: 0.9843 - val_loss: 0.0428 - val_acc: 0.9837\n",
      "Epoch 4/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9858Epoch 00003: val_loss did not improve\n",
      "143614/143614 [==============================] - 354s - loss: 0.0354 - acc: 0.9858 - val_loss: 0.0444 - val_acc: 0.9837\n",
      "Epoch 5/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9872Epoch 00004: val_loss did not improve\n",
      "143614/143614 [==============================] - 353s - loss: 0.0316 - acc: 0.9872 - val_loss: 0.0487 - val_acc: 0.9835\n",
      "Fold 5 loss 0.042830429837680095\n",
      "Predicting results...\n",
      "153164/153164 [==============================] - 33s    \n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0702 - acc: 0.9766Epoch 00000: val_loss improved from inf to 0.04571, saving model to attngru_fold6.h5\n",
      "143614/143614 [==============================] - 362s - loss: 0.0701 - acc: 0.9766 - val_loss: 0.0457 - val_acc: 0.9826\n",
      "Epoch 2/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9830Epoch 00001: val_loss improved from 0.04571 to 0.04314, saving model to attngru_fold6.h5\n",
      "143614/143614 [==============================] - 358s - loss: 0.0450 - acc: 0.9830 - val_loss: 0.0431 - val_acc: 0.9836\n",
      "Epoch 3/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9845Epoch 00002: val_loss improved from 0.04314 to 0.04164, saving model to attngru_fold6.h5\n",
      "143614/143614 [==============================] - 357s - loss: 0.0391 - acc: 0.9845 - val_loss: 0.0416 - val_acc: 0.9842\n",
      "Epoch 4/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9861Epoch 00003: val_loss did not improve\n",
      "143614/143614 [==============================] - 355s - loss: 0.0346 - acc: 0.9861 - val_loss: 0.0456 - val_acc: 0.9838\n",
      "Epoch 5/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9873Epoch 00004: val_loss did not improve\n",
      "143614/143614 [==============================] - 353s - loss: 0.0310 - acc: 0.9873 - val_loss: 0.0479 - val_acc: 0.9823\n",
      "Fold 6 loss 0.041635003312791866\n",
      "Predicting results...\n",
      "153164/153164 [==============================] - 33s    \n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0705 - acc: 0.9765Epoch 00000: val_loss improved from inf to 0.04645, saving model to attngru_fold7.h5\n",
      "143614/143614 [==============================] - 364s - loss: 0.0705 - acc: 0.9765 - val_loss: 0.0465 - val_acc: 0.9826\n",
      "Epoch 2/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9831Epoch 00001: val_loss improved from 0.04645 to 0.04567, saving model to attngru_fold7.h5\n",
      "143614/143614 [==============================] - 355s - loss: 0.0449 - acc: 0.9831 - val_loss: 0.0457 - val_acc: 0.9828\n",
      "Epoch 3/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9845Epoch 00002: val_loss improved from 0.04567 to 0.04449, saving model to attngru_fold7.h5\n",
      "143614/143614 [==============================] - 357s - loss: 0.0394 - acc: 0.9845 - val_loss: 0.0445 - val_acc: 0.9834\n",
      "Epoch 4/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9860Epoch 00003: val_loss did not improve\n",
      "143614/143614 [==============================] - 353s - loss: 0.0349 - acc: 0.9860 - val_loss: 0.0467 - val_acc: 0.9836\n",
      "Epoch 5/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9873Epoch 00004: val_loss did not improve\n",
      "143614/143614 [==============================] - 356s - loss: 0.0311 - acc: 0.9873 - val_loss: 0.0528 - val_acc: 0.9833\n",
      "Fold 7 loss 0.04449247287668859\n",
      "Predicting results...\n",
      "153164/153164 [==============================] - 33s    \n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0716 - acc: 0.9760Epoch 00000: val_loss improved from inf to 0.04551, saving model to attngru_fold8.h5\n",
      "143614/143614 [==============================] - 372s - loss: 0.0716 - acc: 0.9760 - val_loss: 0.0455 - val_acc: 0.9822\n",
      "Epoch 2/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9829Epoch 00001: val_loss improved from 0.04551 to 0.04227, saving model to attngru_fold8.h5\n",
      "143614/143614 [==============================] - 360s - loss: 0.0455 - acc: 0.9829 - val_loss: 0.0423 - val_acc: 0.9837\n",
      "Epoch 3/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0397 - acc: 0.9844Epoch 00002: val_loss improved from 0.04227 to 0.04193, saving model to attngru_fold8.h5\n",
      "143614/143614 [==============================] - 363s - loss: 0.0397 - acc: 0.9844 - val_loss: 0.0419 - val_acc: 0.9838\n",
      "Epoch 4/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9858Epoch 00003: val_loss did not improve\n",
      "143614/143614 [==============================] - 360s - loss: 0.0351 - acc: 0.9858 - val_loss: 0.0452 - val_acc: 0.9840\n",
      "Epoch 5/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9871Epoch 00004: val_loss did not improve\n",
      "143614/143614 [==============================] - 363s - loss: 0.0315 - acc: 0.9871 - val_loss: 0.0506 - val_acc: 0.9838\n",
      "Fold 8 loss 0.04192517572922736\n",
      "Predicting results...\n",
      "153164/153164 [==============================] - 34s    \n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0717 - acc: 0.9759Epoch 00000: val_loss improved from inf to 0.04770, saving model to attngru_fold9.h5\n",
      "143614/143614 [==============================] - 366s - loss: 0.0717 - acc: 0.9759 - val_loss: 0.0477 - val_acc: 0.9822\n",
      "Epoch 2/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0451 - acc: 0.9831Epoch 00001: val_loss improved from 0.04770 to 0.04452, saving model to attngru_fold9.h5\n",
      "143614/143614 [==============================] - 362s - loss: 0.0451 - acc: 0.9831 - val_loss: 0.0445 - val_acc: 0.9834\n",
      "Epoch 3/10\n",
      "143360/143614 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9843Epoch 00002: val_loss did not improve\n",
      "143614/143614 [==============================] - 354s - loss: 0.0399 - acc: 0.9843 - val_loss: 0.0445 - val_acc: 0.9833\n",
      "Epoch 4/10\n",
      "153164/153164 [==============================] - 34s    \n"
     ]
    }
   ],
   "source": [
    "train_folds(data, data_post, y, 10, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'CLASSES' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-523baf955150>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtest_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtest_predicts_am\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_predicts_am\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCLASSES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mtest_predicts_am\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtest_predicts_am\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_predicts_am\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mCLASSES\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CLASSES' is not defined"
     ]
    }
   ],
   "source": [
    "CLASSES = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "print(len(test_predicts_list))\n",
    "test_predicts_am = np.zeros(test_predicts_list[0].shape)\n",
    "\n",
    "for fold_predict in test_predicts_list:\n",
    "    test_predicts_am += fold_predict\n",
    "\n",
    "test_predicts_am = (test_predicts_am / len(test_predicts_list))\n",
    "\n",
    "test_ids = test_df[\"id\"].values\n",
    "test_ids = test_ids.reshape((len(test_ids), 1))\n",
    "\n",
    "test_predicts_am = pd.DataFrame(data=test_predicts_am, columns=CLASSES)\n",
    "test_predicts_am[\"id\"] = test_ids\n",
    "test_predicts_am = test_predicts_am[[\"id\"] + CLASSES]\n",
    "test_predicts_am.to_csv(\"10fold_attngru_am.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicts = np.ones(test_predicts_list[0].shape)\n",
    "\n",
    "for fold_predict in test_predicts_list:\n",
    "    test_predicts *= fold_predict\n",
    "\n",
    "test_predicts **= (1. / len(test_predicts_list))\n",
    "\n",
    "test_ids = test_df[\"id\"].values\n",
    "test_ids = test_ids.reshape((len(test_ids), 1))\n",
    "\n",
    "test_predicts = pd.DataFrame(data=test_predicts, columns=CLASSES)\n",
    "test_predicts[\"id\"] = test_ids\n",
    "test_predicts = test_predicts[[\"id\"] + CLASSES]\n",
    "test_predicts.to_csv(\"10fold_attngru_gm.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
