{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading Keras-2.1.4-py2.py3-none-any.whl (322kB)\n",
      "\u001b[K    100% |████████████████████████████████| 327kB 3.3MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already up-to-date: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras)\n",
      "Collecting six>=1.9.0 (from keras)\n",
      "  Using cached six-1.11.0-py2.py3-none-any.whl\n",
      "Collecting pyyaml (from keras)\n",
      "Collecting numpy>=1.9.1 (from keras)\n",
      "  Downloading numpy-1.14.1-cp36-cp36m-manylinux1_x86_64.whl (12.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 12.2MB 127kB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: six, pyyaml, numpy, keras\n",
      "  Found existing installation: six 1.10.0\n",
      "    Uninstalling six-1.10.0:\n",
      "      Successfully uninstalled six-1.10.0\n",
      "  Found existing installation: PyYAML 3.11\n",
      "    Uninstalling PyYAML-3.11:\n",
      "      Successfully uninstalled PyYAML-3.11\n",
      "  Found existing installation: numpy 1.14.0\n",
      "    Uninstalling numpy-1.14.0:\n",
      "      Successfully uninstalled numpy-1.14.0\n",
      "  Found existing installation: Keras 2.0.6\n",
      "    Uninstalling Keras-2.0.6:\n",
      "      Successfully uninstalled Keras-2.0.6\n",
      "Successfully installed keras-2.1.4 numpy-1.14.1 pyyaml-3.12 six-1.11.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import *\n",
    "from keras.layers import concatenate, CuDNNGRU\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "        \n",
    "# path = '..'\n",
    "# EMBEDDING_FILE=path+'/input/glove.840B.300d.txt'\n",
    "# EMBEDDING_FILE=path+'/input/crawl-300d-2M.vec'\n",
    "# TRAIN_DATA_FILE=path+'/input/train.csv'\n",
    "# TEST_DATA_FILE=path+'/input/test.csv'\n",
    "\n",
    "EMBEDDING_FILE='/public/models/glove/glove.840B.300d.txt'\n",
    "TRAIN_DATA_FILE='/public/toxic_comments/train.csv'\n",
    "TEST_DATA_FILE='/public/toxic_comments/test.csv'\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 150\n",
    "MAX_NB_WORDS = 100000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "num_lstm = 300\n",
    "num_dense = 256\n",
    "rate_drop_lstm = 0.2\n",
    "rate_drop_dense = 0.2\n",
    "\n",
    "act = 'relu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanData(text, stemming = False, lemmatize=False):    \n",
    "    text = text.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+\\-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    if stemming:\n",
    "        st = PorterStemmer()\n",
    "        txt = \" \".join([st.stem(w) for w in text.split()])\n",
    "    if lemmatize:\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        txt = \" \".join([wordnet_lemmatizer.lemmatize(w) for w in text.split()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors\n",
      "Found 2195895 word vectors of glove.\n",
      "-0.01444638 0.47249147\n",
      "Total 2195895 word vectors.\n",
      "Processing text dataset\n",
      "Found 292462 unique tokens\n",
      "Shape of data tensor: (159571, 150)\n",
      "Shape of label tensor: (159571, 6)\n",
      "Shape of test_data tensor: (153164, 150)\n"
     ]
    }
   ],
   "source": [
    "print('Indexing word vectors')\n",
    "\n",
    "count = 0\n",
    "embeddings_index = {}\n",
    "f = open(EMBEDDING_FILE)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = ' '.join(values[:-300])\n",
    "    coefs = np.asarray(values[-300:], dtype='float32')\n",
    "    embeddings_index[word] = coefs.reshape(-1)\n",
    "    coef = embeddings_index[word]\n",
    "f.close()\n",
    "\n",
    "print('Found %d word vectors of glove.' % len(embeddings_index))\n",
    "emb_mean,emb_std = coef.mean(), coef.std()\n",
    "print(emb_mean,emb_std)\n",
    "\n",
    "print('Total %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test_df = pd.read_csv(TEST_DATA_FILE)\n",
    "\n",
    "print('Processing text dataset')\n",
    "\n",
    "train_df['comment_text'] = train_df['comment_text'].map(lambda x: cleanData(x,  stemming = False, \n",
    "                                                                            lemmatize=False))\n",
    "test_df['comment_text'] = test_df['comment_text'].map(lambda x: cleanData(x,  stemming = False, \n",
    "                                                                          lemmatize=False))\n",
    "\n",
    "#Regex to remove all Non-Alpha Numeric and space\n",
    "special_character_removal=re.compile(r'[^a-z\\d ]',re.IGNORECASE)\n",
    "#regex to replace all numerics\n",
    "replace_numbers=re.compile(r'\\d+',re.IGNORECASE)\n",
    "\n",
    "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    #Remove Special Characters\n",
    "    text=special_character_removal.sub('',text)\n",
    "    #Replace Numbers\n",
    "    text=replace_numbers.sub('n',text)\n",
    "\n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    return(text)\n",
    "\n",
    "\n",
    "list_sentences_train = train_df[\"comment_text\"].fillna(\"NA\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train_df[list_classes].values\n",
    "list_sentences_test = test_df[\"comment_text\"].fillna(\"NA\").values\n",
    "\n",
    "\n",
    "comments = []\n",
    "for text in list_sentences_train:\n",
    "    comments.append(text_to_wordlist(text))\n",
    "    \n",
    "test_comments=[]\n",
    "for text in list_sentences_test:\n",
    "    test_comments.append(text_to_wordlist(text))\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(comments + test_comments)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(comments)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_comments)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', y.shape)\n",
    "\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of test_data tensor:', test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (159571, 150)\n",
      "Shape of label tensor: (159571, 6)\n",
      "Shape of test_data tensor: (153164, 150)\n"
     ]
    }
   ],
   "source": [
    "data_post = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH,padding='post', truncating='post')\n",
    "print('Shape of data tensor:', data_post.shape)\n",
    "print('Shape of label tensor:', y.shape)\n",
    "\n",
    "test_data_post = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "print('Shape of test_data tensor:', test_data_post.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: 21603\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix')\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from os.path import dirname\n",
    "from keras import initializers\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "class AttentionWeightedAverage(Layer):\n",
    "    \"\"\"\n",
    "    Computes a weighted average of the different channels across timesteps.\n",
    "    Uses 1 parameter pr. channel to compute the attention value for a single timestep.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, return_attention=False, **kwargs):\n",
    "        self.init = initializers.get('uniform')\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        super(AttentionWeightedAverage, self).__init__(** kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(ndim=3)]\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[2], 1),\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 initializer=self.init)\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttentionWeightedAverage, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # computes a probability distribution over the timesteps\n",
    "        # uses 'max trick' for numerical stability\n",
    "        # reshape is done to avoid issue with Tensorflow\n",
    "        # and 1-dimensional weights\n",
    "        logits = K.dot(x, self.W)\n",
    "        x_shape = K.shape(x)\n",
    "        logits = K.reshape(logits, (x_shape[0], x_shape[1]))\n",
    "        ai = K.exp(logits - K.max(logits, axis=-1, keepdims=True))\n",
    "\n",
    "        # masked timesteps have zero weight\n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            ai = ai * mask\n",
    "        att_weights = ai / (K.sum(ai, axis=1, keepdims=True) + K.epsilon())\n",
    "        weighted_input = x * K.expand_dims(att_weights)\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "        if self.return_attention:\n",
    "            return [result, att_weights]\n",
    "        return result\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return self.compute_output_shape(input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_len = input_shape[2]\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]\n",
    "        return (input_shape[0], output_len)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        if isinstance(input_mask, list):\n",
    "            return [None] * len(input_mask)\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Nadam\n",
    "def deepmoji_architecture(nb_classes, maxlen, feature_output=False, embed_dropout_rate=0, final_dropout_rate=0, embed_l2=1E-6, return_attention=False):\n",
    "    \"\"\"\n",
    "    Returns the DeepMoji architecture uninitialized and\n",
    "    without using the pretrained model weights.\n",
    "    # Arguments:\n",
    "        nb_classes: Number of classes in the dataset.\n",
    "        nb_tokens: Number of tokens in the dataset (i.e. vocabulary size).\n",
    "        maxlen: Maximum length of a token.\n",
    "        feature_output: If True the model returns the penultimate\n",
    "                        feature vector rather than Softmax probabilities\n",
    "                        (defaults to False).\n",
    "        embed_dropout_rate: Dropout rate for the embedding layer.\n",
    "        final_dropout_rate: Dropout rate for the final Softmax layer.\n",
    "        embed_l2: L2 regularization for the embedding layerl.\n",
    "    # Returns:\n",
    "        Model with the given parameters.\n",
    "    \"\"\"\n",
    "    # define embedding layer that turns word tokens into vectors\n",
    "    # an activation function is used to bound the values of the embedding\n",
    "    model_input = Input(shape=(maxlen,), dtype='int32')\n",
    "    model_input_post = Input(shape=(maxlen,), dtype='int32')\n",
    "    \n",
    "    embed_reg = L1L2(l2=embed_l2) if embed_l2 != 0 else None\n",
    "    embed = Embedding(nb_words,\n",
    "        EMBEDDING_DIM,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        embeddings_regularizer=embed_reg,\n",
    "        trainable=False)\n",
    "    x = embed(model_input)\n",
    "    x1 = Activation('tanh')(x)\n",
    "    x2 = Activation('relu')(x)\n",
    "    x = concatenate([x1,x2])\n",
    "\n",
    "    # entire embedding channels are dropped out instead of the\n",
    "    # normal Keras embedding dropout, which drops all channels for entire words\n",
    "    # many of the datasets contain so few words that losing one or more words can alter the emotions completely\n",
    "    if embed_dropout_rate != 0:\n",
    "        embed_drop = SpatialDropout1D(embed_dropout_rate, name='embed_drop')\n",
    "        x = embed_drop(x)\n",
    "\n",
    "    # skip-connection from embedding to output eases gradient-flow and allows access to lower-level features\n",
    "    # ordering of the way the merge is done is important for consistency with the pretrained model\n",
    "    lstm_0_output = Bidirectional(CuDNNGRU(64, return_sequences=True), name=\"bi_gru_0\")(x)\n",
    "    lstm_0_output = Dropout(0.4)(lstm_0_output)\n",
    "    lstm_1_output = Bidirectional(CuDNNGRU(64, return_sequences=True), name=\"bi_gru_1\")(lstm_0_output)\n",
    "    x = concatenate([lstm_1_output, lstm_0_output, x])\n",
    "\n",
    "    # if return_attention is True in AttentionWeightedAverage, an additional tensor\n",
    "    # representing the weight at each timestep is returned\n",
    "    weights = None\n",
    "    x = AttentionWeightedAverage(name='attlayer', return_attention=return_attention)(x)\n",
    "    if return_attention:\n",
    "        x, weights = x\n",
    "\n",
    "    if not feature_output:\n",
    "        # output class probabilities\n",
    "        if final_dropout_rate != 0:\n",
    "            x = Dropout(final_dropout_rate)(x)\n",
    "            \n",
    "    embed_reg = L1L2(l2=embed_l2) if embed_l2 != 0 else None\n",
    "    embed_post = Embedding(nb_words,\n",
    "        EMBEDDING_DIM,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        embeddings_regularizer=embed_reg,\n",
    "        trainable=False)\n",
    "    x_post = embed(model_input_post)\n",
    "    x1_post = Activation('tanh')(x_post)\n",
    "    x2_post = Activation('relu')(x_post)\n",
    "    x_post = concatenate([x1_post,x2_post])\n",
    "\n",
    "    # entire embedding channels are dropped out instead of the\n",
    "    # normal Keras embedding dropout, which drops all channels for entire words\n",
    "    # many of the datasets contain so few words that losing one or more words can alter the emotions completely\n",
    "    if embed_dropout_rate != 0:\n",
    "        embed_drop_post = SpatialDropout1D(embed_dropout_rate, name='embed_drop_p')\n",
    "        x_post = embed_drop_post(x_post)\n",
    "\n",
    "    # skip-connection from embedding to output eases gradient-flow and allows access to lower-level features\n",
    "    # ordering of the way the merge is done is important for consistency with the pretrained model\n",
    "    lstm_0_output_post = Bidirectional(CuDNNGRU(64, return_sequences=True), name=\"bi_gru_0_p\")(x_post)\n",
    "    lstm_0_output_post = Dropout(0.4)(lstm_0_output_post)\n",
    "    lstm_1_output_post = Bidirectional(CuDNNGRU(64, return_sequences=True), name=\"bi_gru_1_p\")(lstm_0_output_post)\n",
    "    x_post = concatenate([lstm_1_output_post, lstm_0_output_post, x_post])\n",
    "\n",
    "    # if return_attention is True in AttentionWeightedAverage, an additional tensor\n",
    "    # representing the weight at each timestep is returned\n",
    "    weights = None\n",
    "    x_post = AttentionWeightedAverage(name='attlayer_p', return_attention=return_attention)(x_post)\n",
    "    if return_attention:\n",
    "        x_post, weights = x_post\n",
    "\n",
    "    if not feature_output:\n",
    "        # output class probabilities\n",
    "        if final_dropout_rate != 0:\n",
    "            x_post = Dropout(final_dropout_rate)(x_post)\n",
    "            \n",
    "    x_all = concatenate([x,x_post])\n",
    "    \n",
    "    if not feature_output:\n",
    "        # output class probabilities\n",
    "        if nb_classes > 2:\n",
    "            outputs = [Dense(nb_classes, activation='sigmoid', name='sigmoid_multiclass')(x_all)]\n",
    "        else:\n",
    "            outputs = [Dense(1, activation='sigmoid', name='sigmoid_single_class')(x_all)]\n",
    "    else:\n",
    "        # output penultimate feature vector\n",
    "        outputs = [x]\n",
    "\n",
    "    if return_attention:\n",
    "        # add the attention weights to the outputs if required\n",
    "        outputs.append(weights)\n",
    "        \n",
    "    model = Model(inputs=[model_input,model_input_post], outputs=outputs, name=\"DeepMoji\")\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "        optimizer='Adam',\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.optimizers import Adam\n",
    "\n",
    "# def get_model():\n",
    "#     comment_input = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "\n",
    "#     emb = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix], \n",
    "#                   input_length=MAX_SEQUENCE_LENGTH)(comment_input)\n",
    "#     emb = SpatialDropout1D(0.3)(emb)\n",
    "# #     x = Bidirectional(GRU(100, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))(emb)\n",
    "#     x = Bidirectional(CuDNNGRU(100, return_sequences=True))(emb)\n",
    "#     x = Attention(MAX_SEQUENCE_LENGTH)(x)\n",
    "#     x = Dense(75, activation=\"relu\")(x)\n",
    "#     x = Dropout(0.2)(x)\n",
    "#     preds = Dense(6, activation=\"sigmoid\")(x)\n",
    "#     model = Model(inputs=[comment_input], outputs=preds)\n",
    "#     model.compile(loss='binary_crossentropy', optimizer=Adam(clipvalue=1, clipnorm=1), metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "import numpy as np\n",
    "\n",
    "test_predicts_list = []\n",
    "\n",
    "def train_folds(data,data_post, y, fold_count, batch_size):\n",
    "    print(\"Starting to train models...\")\n",
    "    fold_size = len(data) // fold_count\n",
    "    models = []\n",
    "    for fold_id in range(6, fold_count):\n",
    "        fold_start = fold_size * fold_id\n",
    "        fold_end = fold_start + fold_size\n",
    "\n",
    "        if fold_id == fold_size - 1:\n",
    "            fold_end = len(data)\n",
    "\n",
    "        print(\"Fold {0}\".format(fold_id))\n",
    "        \n",
    "        train_x = np.concatenate([data[:fold_start], data[fold_end:]])\n",
    "        train_xp = np.concatenate([data_post[:fold_start], data_post[fold_end:]])\n",
    "        train_y = np.concatenate([y[:fold_start], y[fold_end:]])\n",
    "\n",
    "        val_x = data[fold_start:fold_end]\n",
    "        val_xp = data_post[fold_start:fold_end]\n",
    "        val_y = y[fold_start:fold_end]\n",
    "        \n",
    "        file_path=\"attngru_fold{0}.h5\".format(fold_id)\n",
    "#         model = get_model()\n",
    "        model = deepmoji_architecture(6, MAX_SEQUENCE_LENGTH, feature_output=False, embed_dropout_rate=0.4, final_dropout_rate=0.4, embed_l2=0, \n",
    "                              return_attention=False)\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3)\n",
    "        callbacks_list = [checkpoint, early] \n",
    "\n",
    "        hist = model.fit([train_x, train_xp], train_y, epochs=15, batch_size=128, shuffle=True, \n",
    "                         validation_data=([val_x, val_xp], val_y), callbacks = callbacks_list, verbose=1)\n",
    "        model.load_weights(file_path)\n",
    "        best_score = min(hist.history['val_loss'])\n",
    "        \n",
    "        print(\"Fold {0} loss {1}\".format(fold_id, best_score))\n",
    "        print(\"Predicting validation...\")\n",
    "        val_predicts_path = \"dm_val_predicts{0}.npy\".format(fold_id)\n",
    "        val_predicts = model.predict([val_x, val_xp], batch_size=1024, verbose=1)\n",
    "        np.save(val_predicts_path, val_predicts)\n",
    "        \n",
    "        print(\"Predicting results...\")\n",
    "        test_predicts_path = \"dm_test_predicts{0}.npy\".format(fold_id)\n",
    "        test_predicts = model.predict([test_data, test_data_post], batch_size=1024, verbose=1)\n",
    "        test_predicts_list.append(test_predicts)\n",
    "        np.save(test_predicts_path, test_predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train models...\n",
      "Fold 0\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/15\n",
      "143614/143614 [==============================] - 332s 2ms/step - loss: 0.0582 - acc: 0.9797 - val_loss: 0.0428 - val_acc: 0.9828\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04283, saving model to attngru_fold0.h5\n",
      "Epoch 2/15\n",
      "143614/143614 [==============================] - 327s 2ms/step - loss: 0.0437 - acc: 0.9831 - val_loss: 0.0404 - val_acc: 0.9839\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04283 to 0.04038, saving model to attngru_fold0.h5\n",
      "Epoch 3/15\n",
      "143614/143614 [==============================] - 329s 2ms/step - loss: 0.0414 - acc: 0.9837 - val_loss: 0.0392 - val_acc: 0.9845\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04038 to 0.03923, saving model to attngru_fold0.h5\n",
      "Epoch 4/15\n",
      "143614/143614 [==============================] - 329s 2ms/step - loss: 0.0398 - acc: 0.9843 - val_loss: 0.0393 - val_acc: 0.9844\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/15\n",
      "143614/143614 [==============================] - 330s 2ms/step - loss: 0.0384 - acc: 0.9847 - val_loss: 0.0384 - val_acc: 0.9848\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.03923 to 0.03840, saving model to attngru_fold0.h5\n",
      "Epoch 6/15\n",
      "143614/143614 [==============================] - 327s 2ms/step - loss: 0.0372 - acc: 0.9851 - val_loss: 0.0399 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/15\n",
      "143614/143614 [==============================] - 327s 2ms/step - loss: 0.0360 - acc: 0.9855 - val_loss: 0.0391 - val_acc: 0.9847\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/15\n",
      "143614/143614 [==============================] - 329s 2ms/step - loss: 0.0348 - acc: 0.9859 - val_loss: 0.0396 - val_acc: 0.9846\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Fold 0 loss 0.038401426950433075\n",
      "Predicting results...\n",
      "153164/153164 [==============================] - 55s 359us/step\n",
      "Fold 1\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/15\n",
      "143614/143614 [==============================] - 330s 2ms/step - loss: 0.0581 - acc: 0.9798 - val_loss: 0.0429 - val_acc: 0.9829\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04286, saving model to attngru_fold1.h5\n",
      "Epoch 2/15\n",
      "143614/143614 [==============================] - 329s 2ms/step - loss: 0.0436 - acc: 0.9832 - val_loss: 0.0417 - val_acc: 0.9832\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04286 to 0.04173, saving model to attngru_fold1.h5\n",
      "Epoch 3/15\n",
      "143614/143614 [==============================] - 329s 2ms/step - loss: 0.0411 - acc: 0.9839 - val_loss: 0.0409 - val_acc: 0.9838\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04173 to 0.04088, saving model to attngru_fold1.h5\n",
      "Epoch 4/15\n",
      "143614/143614 [==============================] - 329s 2ms/step - loss: 0.0398 - acc: 0.9844 - val_loss: 0.0401 - val_acc: 0.9839\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04088 to 0.04011, saving model to attngru_fold1.h5\n",
      "Epoch 5/15\n",
      "143614/143614 [==============================] - 329s 2ms/step - loss: 0.0381 - acc: 0.9848 - val_loss: 0.0401 - val_acc: 0.9838\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/15\n",
      "143614/143614 [==============================] - 329s 2ms/step - loss: 0.0371 - acc: 0.9853 - val_loss: 0.0400 - val_acc: 0.9839\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.04011 to 0.03996, saving model to attngru_fold1.h5\n",
      "Epoch 7/15\n",
      "143614/143614 [==============================] - 330s 2ms/step - loss: 0.0358 - acc: 0.9856 - val_loss: 0.0398 - val_acc: 0.9841\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.03996 to 0.03980, saving model to attngru_fold1.h5\n",
      "Epoch 8/15\n",
      "143614/143614 [==============================] - 329s 2ms/step - loss: 0.0347 - acc: 0.9860 - val_loss: 0.0412 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/15\n",
      "143614/143614 [==============================] - 329s 2ms/step - loss: 0.0336 - acc: 0.9864 - val_loss: 0.0410 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/15\n",
      "143614/143614 [==============================] - 330s 2ms/step - loss: 0.0326 - acc: 0.9868 - val_loss: 0.0407 - val_acc: 0.9841\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Fold 1 loss 0.039801436078421806\n",
      "Predicting results...\n",
      "153164/153164 [==============================] - 55s 358us/step\n",
      "Fold 2\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/15\n",
      "143614/143614 [==============================] - 331s 2ms/step - loss: 0.0585 - acc: 0.9794 - val_loss: 0.0419 - val_acc: 0.9837\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04193, saving model to attngru_fold2.h5\n",
      "Epoch 2/15\n",
      "143614/143614 [==============================] - 329s 2ms/step - loss: 0.0435 - acc: 0.9831 - val_loss: 0.0406 - val_acc: 0.9843\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04193 to 0.04064, saving model to attngru_fold2.h5\n",
      "Epoch 3/15\n",
      "143614/143614 [==============================] - 329s 2ms/step - loss: 0.0414 - acc: 0.9838 - val_loss: 0.0392 - val_acc: 0.9843\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04064 to 0.03917, saving model to attngru_fold2.h5\n",
      "Epoch 4/15\n",
      "143614/143614 [==============================] - 330s 2ms/step - loss: 0.0397 - acc: 0.9844 - val_loss: 0.0393 - val_acc: 0.9845\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/15\n",
      "143614/143614 [==============================] - 329s 2ms/step - loss: 0.0384 - acc: 0.9848 - val_loss: 0.0382 - val_acc: 0.9849\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.03917 to 0.03821, saving model to attngru_fold2.h5\n",
      "Epoch 6/15\n",
      "  2944/143614 [..............................] - ETA: 5:08 - loss: 0.0339 - acc: 0.9871Websocket connection lost. Retrying...\n",
      "  3200/143614 [..............................] - ETA: 5:08 - loss: 0.0347 - acc: 0.9869Websocket connection lost. Retrying...\n",
      " 16896/143614 [==>...........................] - ETA: 4:38 - loss: 0.0380 - acc: 0.9852Websocket connection restored!\n",
      "143614/143614 [==============================] - 330s 2ms/step - loss: 0.0372 - acc: 0.9851 - val_loss: 0.0388 - val_acc: 0.9849\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/15\n",
      "143614/143614 [==============================] - 329s 2ms/step - loss: 0.0363 - acc: 0.9854 - val_loss: 0.0382 - val_acc: 0.9847\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.03821 to 0.03817, saving model to attngru_fold2.h5\n",
      "Epoch 8/15\n",
      "143614/143614 [==============================] - 329s 2ms/step - loss: 0.0350 - acc: 0.9860 - val_loss: 0.0389 - val_acc: 0.9847\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/15\n",
      "143614/143614 [==============================] - 330s 2ms/step - loss: 0.0339 - acc: 0.9863 - val_loss: 0.0394 - val_acc: 0.9846\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/15\n",
      "143614/143614 [==============================] - 330s 2ms/step - loss: 0.0328 - acc: 0.9868 - val_loss: 0.0396 - val_acc: 0.9844\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Fold 2 loss 0.03816563602446568\n",
      "Predicting results...\n",
      "153164/153164 [==============================] - 55s 359us/step\n",
      "Fold 3\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/15\n",
      "143614/143614 [==============================] - 332s 2ms/step - loss: 0.0587 - acc: 0.9794 - val_loss: 0.0444 - val_acc: 0.9831\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04438, saving model to attngru_fold3.h5\n",
      "Epoch 2/15\n",
      "143614/143614 [==============================] - 330s 2ms/step - loss: 0.0433 - acc: 0.9832 - val_loss: 0.0418 - val_acc: 0.9841\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04438 to 0.04176, saving model to attngru_fold3.h5\n",
      "Epoch 3/15\n",
      "143614/143614 [==============================] - 328s 2ms/step - loss: 0.0412 - acc: 0.9839 - val_loss: 0.0418 - val_acc: 0.9842\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/15\n",
      "143614/143614 [==============================] - 329s 2ms/step - loss: 0.0397 - acc: 0.9843 - val_loss: 0.0419 - val_acc: 0.9843\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/15\n",
      "143614/143614 [==============================] - 329s 2ms/step - loss: 0.0383 - acc: 0.9847 - val_loss: 0.0407 - val_acc: 0.9846\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.04176 to 0.04074, saving model to attngru_fold3.h5\n",
      "Epoch 6/15\n",
      "143614/143614 [==============================] - 329s 2ms/step - loss: 0.0372 - acc: 0.9851 - val_loss: 0.0408 - val_acc: 0.9841\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/15\n",
      "143614/143614 [==============================] - 329s 2ms/step - loss: 0.0358 - acc: 0.9856 - val_loss: 0.0412 - val_acc: 0.9838\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/15\n",
      "143614/143614 [==============================] - 329s 2ms/step - loss: 0.0347 - acc: 0.9860 - val_loss: 0.0417 - val_acc: 0.9835\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Fold 3 loss 0.040744778137480606\n",
      "Predicting results...\n",
      "153164/153164 [==============================] - 55s 360us/step\n",
      "Fold 4\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/15\n",
      "143614/143614 [==============================] - 331s 2ms/step - loss: 0.0582 - acc: 0.9796 - val_loss: 0.0431 - val_acc: 0.9831\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04305, saving model to attngru_fold4.h5\n",
      "Epoch 2/15\n",
      "143614/143614 [==============================] - 329s 2ms/step - loss: 0.0439 - acc: 0.9829 - val_loss: 0.0405 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04305 to 0.04046, saving model to attngru_fold4.h5\n",
      "Epoch 3/15\n",
      "143614/143614 [==============================] - 330s 2ms/step - loss: 0.0416 - acc: 0.9838 - val_loss: 0.0398 - val_acc: 0.9844\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04046 to 0.03978, saving model to attngru_fold4.h5\n",
      "Epoch 4/15\n",
      "143614/143614 [==============================] - 330s 2ms/step - loss: 0.0401 - acc: 0.9843 - val_loss: 0.0393 - val_acc: 0.9850\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.03978 to 0.03927, saving model to attngru_fold4.h5\n",
      "Epoch 5/15\n",
      "143614/143614 [==============================] - 329s 2ms/step - loss: 0.0388 - acc: 0.9846 - val_loss: 0.0392 - val_acc: 0.9852\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.03927 to 0.03916, saving model to attngru_fold4.h5\n",
      "Epoch 6/15\n",
      "143614/143614 [==============================] - 329s 2ms/step - loss: 0.0379 - acc: 0.9848 - val_loss: 0.0397 - val_acc: 0.9848\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/15\n",
      "143614/143614 [==============================] - 329s 2ms/step - loss: 0.0364 - acc: 0.9853 - val_loss: 0.0385 - val_acc: 0.9852\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.03916 to 0.03845, saving model to attngru_fold4.h5\n",
      "Epoch 8/15\n",
      "143614/143614 [==============================] - 330s 2ms/step - loss: 0.0353 - acc: 0.9858 - val_loss: 0.0391 - val_acc: 0.9848\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/15\n",
      "143614/143614 [==============================] - 330s 2ms/step - loss: 0.0345 - acc: 0.9860 - val_loss: 0.0404 - val_acc: 0.9843\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/15\n",
      "143614/143614 [==============================] - 329s 2ms/step - loss: 0.0334 - acc: 0.9864 - val_loss: 0.0395 - val_acc: 0.9852\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Fold 4 loss 0.03845395548788013\n",
      "Predicting results...\n",
      "153164/153164 [==============================] - 55s 362us/step\n",
      "Fold 5\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/15\n",
      "143614/143614 [==============================] - 332s 2ms/step - loss: 0.0579 - acc: 0.9800 - val_loss: 0.0414 - val_acc: 0.9841\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04143, saving model to attngru_fold5.h5\n",
      "Epoch 2/15\n",
      "143614/143614 [==============================] - 330s 2ms/step - loss: 0.0440 - acc: 0.9829 - val_loss: 0.0389 - val_acc: 0.9849\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04143 to 0.03892, saving model to attngru_fold5.h5\n",
      "Epoch 3/15\n",
      "143614/143614 [==============================] - 330s 2ms/step - loss: 0.0416 - acc: 0.9836 - val_loss: 0.0384 - val_acc: 0.9849\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.03892 to 0.03845, saving model to attngru_fold5.h5\n",
      "Epoch 4/15\n",
      "143614/143614 [==============================] - 330s 2ms/step - loss: 0.0400 - acc: 0.9842 - val_loss: 0.0393 - val_acc: 0.9844\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/15\n",
      "143614/143614 [==============================] - 329s 2ms/step - loss: 0.0387 - acc: 0.9846 - val_loss: 0.0386 - val_acc: 0.9845\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/15\n",
      "143614/143614 [==============================] - 329s 2ms/step - loss: 0.0375 - acc: 0.9852 - val_loss: 0.0383 - val_acc: 0.9849\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.03845 to 0.03834, saving model to attngru_fold5.h5\n",
      "Epoch 7/15\n",
      "143614/143614 [==============================] - 330s 2ms/step - loss: 0.0364 - acc: 0.9853 - val_loss: 0.0388 - val_acc: 0.9852\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/15\n",
      "143614/143614 [==============================] - 330s 2ms/step - loss: 0.0350 - acc: 0.9860 - val_loss: 0.0385 - val_acc: 0.9847\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/15\n",
      "143614/143614 [==============================] - 331s 2ms/step - loss: 0.0340 - acc: 0.9864 - val_loss: 0.0385 - val_acc: 0.9849\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Fold 5 loss 0.0383406596742358\n",
      "Predicting results...\n",
      "153164/153164 [==============================] - 55s 361us/step\n",
      "Fold 6\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/15\n",
      "109184/143614 [=====================>........] - ETA: 1:16 - loss: 0.0618 - acc: 0.9787"
     ]
    }
   ],
   "source": [
    "train_folds(data, data_post, y, 10, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "print(len(test_predicts_list))\n",
    "test_predicts_am = np.zeros(test_predicts_list[0].shape)\n",
    "\n",
    "for fold_predict in test_predicts_list:\n",
    "    test_predicts_am += fold_predict\n",
    "\n",
    "test_predicts_am = (test_predicts_am / len(test_predicts_list))\n",
    "\n",
    "test_ids = test_df[\"id\"].values\n",
    "test_ids = test_ids.reshape((len(test_ids), 1))\n",
    "\n",
    "test_predicts_am = pd.DataFrame(data=test_predicts_am, columns=CLASSES)\n",
    "test_predicts_am[\"id\"] = test_ids\n",
    "test_predicts_am = test_predicts_am[[\"id\"] + CLASSES]\n",
    "test_predicts_am.to_csv(\"10fold_deepmoji_am.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
