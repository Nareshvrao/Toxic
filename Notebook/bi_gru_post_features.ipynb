{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading Keras-2.1.4-py2.py3-none-any.whl (322kB)\n",
      "\u001b[K    100% |████████████████████████████████| 327kB 3.2MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already up-to-date: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras)\n",
      "Collecting numpy>=1.9.1 (from keras)\n",
      "  Downloading numpy-1.14.1-cp36-cp36m-manylinux1_x86_64.whl (12.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 12.2MB 123kB/s eta 0:00:01   52% |████████████████▉               | 6.4MB 53.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyyaml (from keras)\n",
      "Collecting six>=1.9.0 (from keras)\n",
      "  Using cached six-1.11.0-py2.py3-none-any.whl\n",
      "Installing collected packages: numpy, pyyaml, six, keras\n",
      "  Found existing installation: numpy 1.14.0\n",
      "    Uninstalling numpy-1.14.0:\n",
      "      Successfully uninstalled numpy-1.14.0\n",
      "  Found existing installation: PyYAML 3.11\n",
      "    Uninstalling PyYAML-3.11:\n",
      "      Successfully uninstalled PyYAML-3.11\n",
      "  Found existing installation: six 1.10.0\n",
      "    Uninstalling six-1.10.0:\n",
      "      Successfully uninstalled six-1.10.0\n",
      "  Found existing installation: Keras 2.0.6\n",
      "    Uninstalling Keras-2.0.6:\n",
      "      Successfully uninstalled Keras-2.0.6\n",
      "Successfully installed keras-2.1.4 numpy-1.14.1 pyyaml-3.12 six-1.11.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, string\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import *\n",
    "from keras.layers import concatenate, CuDNNGRU\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import sys\n",
    "stopwords = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        #self.init = initializations.get('glorot_uniform')\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # eij = K.dot(x, self.W) TF backend doesn't support it\n",
    "\n",
    "        # features_dim = self.W.shape[0]\n",
    "        # step_dim = x._keras_shape[1]\n",
    "\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "    #print weigthted_input.shape\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        #return input_shape[0], input_shape[-1]\n",
    "        return input_shape[0],  self.features_dim\n",
    "        \n",
    "# path = '..'\n",
    "# EMBEDDING_FILE=path+'/input/glove.840B.300d.txt'\n",
    "# EMBEDDING_FILE=path+'/input/crawl-300d-2M.vec'\n",
    "# TRAIN_DATA_FILE=path+'/input/train.csv'\n",
    "# TEST_DATA_FILE=path+'/input/test.csv'\n",
    "\n",
    "# EMBEDDING_FILE='/public/models/fasttext/crawl-300d-2M.vec'\n",
    "EMBEDDING_FILE='/public/models/glove/glove.840B.300d.txt'\n",
    "TRAIN_DATA_FILE='/public/toxic_comments/train.csv'\n",
    "TEST_DATA_FILE='/public/toxic_comments/test.csv'\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "MAX_NB_WORDS = 100000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "num_lstm = 300\n",
    "num_dense = 256\n",
    "rate_drop_lstm = 0.2\n",
    "rate_drop_dense = 0.2\n",
    "\n",
    "act = 'relu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanData(text, stemming = False, lemmatize=False):    \n",
    "    text = text.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+\\-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    if stemming:\n",
    "        st = PorterStemmer()\n",
    "        txt = \" \".join([st.stem(w) for w in text.split()])\n",
    "    if lemmatize:\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        txt = \" \".join([wordnet_lemmatizer.lemmatize(w) for w in text.split()])\n",
    "    return text\n",
    "\n",
    "\n",
    "cols_f = ['count_sent', 'count_word', 'count_unique_word', 'count_letters', 'count_punctuations', \n",
    "          'count_stopwords', 'mean_word_len', 'word_unique_percent', 'punct_percent', 'num_exclamation_marks'\n",
    "          , 'num_question_marks', 'you_count']\n",
    "cols_mm = ['count_sent', 'count_word', 'count_unique_word', 'count_letters', 'count_punctuations', \n",
    "           'count_stopwords', 'num_exclamation_marks', 'num_question_marks', 'you_count']\n",
    "\n",
    "def get_features(df):\n",
    "    df['count_sent']=df[\"comment_text\"].apply(lambda x: len(re.findall(\"\\n\",str(x)))+1)\n",
    "    df['count_word']=df[\"comment_text\"].apply(lambda x: len(str(x).split()))\n",
    "    df['count_unique_word']=df[\"comment_text\"].apply(lambda x: len(set(str(x).split())))\n",
    "    df['count_letters']=df[\"comment_text\"].apply(lambda x: len(str(x)))\n",
    "    df[\"count_punctuations\"] =df[\"comment_text\"].apply(lambda x: len([c for c in str(x) if c in \n",
    "                                                                      string.punctuation]))\n",
    "    df[\"count_stopwords\"] = df[\"comment_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stopwords]))\n",
    "    df[\"mean_word_len\"] = df[\"comment_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "    df['word_unique_percent']=df['count_unique_word']*100/df['count_word']\n",
    "    df['punct_percent']=df['count_punctuations']*100/df['count_word']\n",
    "    df['num_exclamation_marks'] = df['comment_text'].apply(lambda comment: comment.count('!'))\n",
    "    df['num_question_marks'] = df['comment_text'].apply(lambda comment: comment.count('?'))\n",
    "    df['you_count'] = df['comment_text'].apply(lambda comment: sum(comment.count(w) for w in ('you', 'You', 'YOU')))\n",
    "    scaler = MinMaxScaler().fit(df[cols_mm])\n",
    "    df[cols_mm] = scaler.transform(df[cols_mm])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors\n",
      "Found 2195895 word vectors of glove.\n",
      "-0.01444638 0.47249147\n",
      "Total 2195895 word vectors.\n",
      "Extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7945ee554587>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Extracting features'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Processing text dataset'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-2194ff81a664>\u001b[0m in \u001b[0;36mget_features\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_question_marks'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'comment_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mcomment\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcomment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'?'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'you_count'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'comment_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mcomment\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'you'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'You'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'YOU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcols_mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcols_mm\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcols_mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         X = check_array(X, copy=self.copy, warn_on_dtype=True,\n\u001b[0;32m--> 334\u001b[0;31m                         estimator=self, dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0mdata_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    451\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     42\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     43\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 44\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "print('Indexing word vectors')\n",
    "\n",
    "count = 0\n",
    "embeddings_index = {}\n",
    "f = open(EMBEDDING_FILE)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = ' '.join(values[:-300])\n",
    "    coefs = np.asarray(values[-300:], dtype='float32')\n",
    "    embeddings_index[word] = coefs.reshape(-1)\n",
    "    coef = embeddings_index[word]\n",
    "f.close()\n",
    "\n",
    "print('Found %d word vectors of glove.' % len(embeddings_index))\n",
    "emb_mean,emb_std = coef.mean(), coef.std()\n",
    "print(emb_mean,emb_std)\n",
    "\n",
    "print('Total %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n",
      "Found 292462 unique tokens\n",
      "Shape of data tensor: (159571, 100)\n",
      "Shape of label tensor: (159571, 6)\n",
      "Shape of test_data tensor: (153164, 100)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test_df = pd.read_csv(TEST_DATA_FILE)\n",
    "\n",
    "print ('Extracting features')\n",
    "train_df = get_features(train_df)\n",
    "test_df = get_features(test_df)\n",
    "\n",
    "print('Processing text dataset')\n",
    "\n",
    "train_df['comment_text'] = train_df['comment_text'].map(lambda x: cleanData(x,  stemming = False, \n",
    "                                                                            lemmatize=False))\n",
    "test_df['comment_text'] = test_df['comment_text'].map(lambda x: cleanData(x,  stemming = False, \n",
    "                                                                          lemmatize=False))\n",
    "\n",
    "#Regex to remove all Non-Alpha Numeric and space\n",
    "special_character_removal=re.compile(r'[^a-z\\d ]',re.IGNORECASE)\n",
    "#regex to replace all numerics\n",
    "replace_numbers=re.compile(r'\\d+',re.IGNORECASE)\n",
    "\n",
    "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    #Remove Special Characters\n",
    "    text=special_character_removal.sub('',text)\n",
    "    #Replace Numbers\n",
    "    text=replace_numbers.sub('n',text)\n",
    "\n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    return(text)\n",
    "\n",
    "\n",
    "list_sentences_train = train_df[\"comment_text\"].fillna(\"NA\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train_df[list_classes].values\n",
    "list_sentences_test = test_df[\"comment_text\"].fillna(\"NA\").values\n",
    "\n",
    "\n",
    "comments = []\n",
    "for text in list_sentences_train:\n",
    "    comments.append(text_to_wordlist(text))\n",
    "    \n",
    "test_comments=[]\n",
    "for text in list_sentences_test:\n",
    "    test_comments.append(text_to_wordlist(text))\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(comments + test_comments)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(comments)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_comments)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', y.shape)\n",
    "\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of test_data tensor:', test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (159571, 100)\n",
      "Shape of label tensor: (159571, 6)\n",
      "Shape of test_data tensor: (153164, 100)\n"
     ]
    }
   ],
   "source": [
    "data_post = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH,padding='post', truncating='post')\n",
    "print('Shape of data tensor:', data_post.shape)\n",
    "print('Shape of label tensor:', y.shape)\n",
    "\n",
    "test_data_post = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "print('Shape of test_data tensor:', test_data_post.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: 21603\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix')\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "def get_model():\n",
    "    comment_input = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "    comment_input_post = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "    features = Input(shape=(len(cols_f),))\n",
    "    \n",
    "    x1 = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix], \n",
    "                  input_length=MAX_SEQUENCE_LENGTH)(comment_input)\n",
    "    x1 = SpatialDropout1D(0.4)(x1)\n",
    "    x1 = Bidirectional(CuDNNGRU(80, return_sequences=True))(x1)\n",
    "    avg_pool1 = GlobalAveragePooling1D()(x1)\n",
    "    max_pool1 = GlobalMaxPooling1D()(x1)\n",
    "    \n",
    "    x2 = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix], \n",
    "                  input_length=MAX_SEQUENCE_LENGTH)(comment_input_post)\n",
    "    x2 = SpatialDropout1D(0.4)(x2)\n",
    "    x2 = Bidirectional(CuDNNGRU(80, return_sequences=True))(x2)\n",
    "    avg_pool2 = GlobalAveragePooling1D()(x2)\n",
    "    max_pool2 = GlobalMaxPooling1D()(x2)\n",
    "\n",
    "    conc = concatenate([avg_pool1, max_pool1, avg_pool2, max_pool2, features])\n",
    "    conc = Dropout(0.2)(conc)\n",
    "    preds = Dense(6, activation=\"sigmoid\")(conc)\n",
    "    model = Model(inputs=[comment_input, comment_input_post, features], outputs=preds)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(clipvalue=1, clipnorm=1), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "import numpy as np\n",
    "\n",
    "test_predicts_list = []\n",
    "\n",
    "def train_folds(data, data_post, feat, y, fold_count, batch_size):\n",
    "    print(\"Starting to train models...\")\n",
    "    fold_size = len(data) // fold_count\n",
    "    models = []\n",
    "    for fold_id in range(0, fold_count):\n",
    "        fold_start = fold_size * fold_id\n",
    "        fold_end = fold_start + fold_size\n",
    "\n",
    "        if fold_id == fold_size - 1:\n",
    "            fold_end = len(data)\n",
    "\n",
    "        print(\"Fold {0}\".format(fold_id))\n",
    "        \n",
    "        train_x = np.concatenate([data[:fold_start], data[fold_end:]])\n",
    "        train_xp = np.concatenate([data_post[:fold_start], data_post[fold_end:]])\n",
    "        train_f = np.concatenate([feat[:fold_start], feat[fold_end:]])\n",
    "        train_y = np.concatenate([y[:fold_start], y[fold_end:]])\n",
    "\n",
    "        val_x = data[fold_start:fold_end]\n",
    "        val_xp = data_post[fold_start:fold_end]\n",
    "        val_f = feat[fold_start:fold_end]\n",
    "        val_y = y[fold_start:fold_end]\n",
    "        \n",
    "        file_path=\"attngru_pp_fold{0}.h5\".format(fold_id)\n",
    "        model = get_model()\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=0)\n",
    "        callbacks_list = [checkpoint, early] \n",
    "\n",
    "        hist = model.fit([train_x, train_xp, train_f], train_y, epochs=4, batch_size=256, shuffle=True, \n",
    "                         validation_data=([val_x, val_xp, val_f], val_y), callbacks = callbacks_list, verbose=1)\n",
    "        model.load_weights(file_path)\n",
    "        best_score = min(hist.history['val_loss'])\n",
    "        \n",
    "        print(\"Fold {0} loss {1}\".format(fold_id, best_score))\n",
    "        print(\"Predicting results...\")\n",
    "        test_predicts_path = \"attngru_pp_test_predicts{0}.npy\".format(fold_id)\n",
    "        test_predicts = model.predict([test_data, test_data_post, test_df[cols_f].as_matrix()], \n",
    "                                      batch_size=1024, verbose=1)\n",
    "        test_predicts_list.append(test_predicts)\n",
    "        np.save(test_predicts_path, test_predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train models...\n",
      "Fold 0\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/4\n",
      "143614/143614 [==============================] - 128s 893us/step - loss: 0.0997 - acc: 0.9699 - val_loss: 0.0423 - val_acc: 0.9838\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04235, saving model to attngru_pp_fold0.h5\n",
      "Epoch 2/4\n",
      "143614/143614 [==============================] - 125s 872us/step - loss: 0.0414 - acc: 0.9840 - val_loss: 0.0399 - val_acc: 0.9845\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04235 to 0.03987, saving model to attngru_pp_fold0.h5\n",
      "Epoch 3/4\n",
      "143614/143614 [==============================] - 125s 871us/step - loss: 0.0367 - acc: 0.9857 - val_loss: 0.0397 - val_acc: 0.9846\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.03987 to 0.03972, saving model to attngru_pp_fold0.h5\n",
      "Epoch 4/4\n",
      "143614/143614 [==============================] - 125s 870us/step - loss: 0.0326 - acc: 0.9869 - val_loss: 0.0414 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Fold 0 loss 0.03972371129829528\n",
      "Predicting results...\n",
      "153164/153164 [==============================] - 18s 116us/step\n",
      "Fold 1\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/4\n",
      "143614/143614 [==============================] - 127s 887us/step - loss: 0.0907 - acc: 0.9724 - val_loss: 0.0428 - val_acc: 0.9832\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04284, saving model to attngru_pp_fold1.h5\n",
      "Epoch 2/4\n",
      "143614/143614 [==============================] - 125s 872us/step - loss: 0.0411 - acc: 0.9841 - val_loss: 0.0405 - val_acc: 0.9839\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04284 to 0.04055, saving model to attngru_pp_fold1.h5\n",
      "Epoch 3/4\n",
      "143614/143614 [==============================] - 125s 873us/step - loss: 0.0368 - acc: 0.9855 - val_loss: 0.0412 - val_acc: 0.9833\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Fold 1 loss 0.040548945612822095\n",
      "Predicting results...\n",
      "153164/153164 [==============================] - 18s 116us/step\n",
      "Fold 2\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/4\n",
      "143614/143614 [==============================] - 127s 884us/step - loss: 0.0981 - acc: 0.9675 - val_loss: 0.0428 - val_acc: 0.9838\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04280, saving model to attngru_pp_fold2.h5\n",
      "Epoch 2/4\n",
      "143614/143614 [==============================] - 125s 872us/step - loss: 0.0414 - acc: 0.9840 - val_loss: 0.0402 - val_acc: 0.9842\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04280 to 0.04018, saving model to attngru_pp_fold2.h5\n",
      "Epoch 3/4\n",
      "143614/143614 [==============================] - 125s 871us/step - loss: 0.0368 - acc: 0.9854 - val_loss: 0.0404 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Fold 2 loss 0.04017528008251491\n",
      "Predicting results...\n",
      "153164/153164 [==============================] - 18s 116us/step\n",
      "Fold 3\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/4\n",
      "143614/143614 [==============================] - 127s 887us/step - loss: 0.1162 - acc: 0.9685 - val_loss: 0.0453 - val_acc: 0.9828\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04527, saving model to attngru_pp_fold3.h5\n",
      "Epoch 2/4\n",
      "143614/143614 [==============================] - 125s 872us/step - loss: 0.0409 - acc: 0.9841 - val_loss: 0.0416 - val_acc: 0.9838\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04527 to 0.04159, saving model to attngru_pp_fold3.h5\n",
      "Epoch 3/4\n",
      "143614/143614 [==============================] - 125s 873us/step - loss: 0.0367 - acc: 0.9856 - val_loss: 0.0411 - val_acc: 0.9841\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04159 to 0.04109, saving model to attngru_pp_fold3.h5\n",
      "Epoch 4/4\n",
      "143614/143614 [==============================] - 125s 873us/step - loss: 0.0326 - acc: 0.9870 - val_loss: 0.0423 - val_acc: 0.9833\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Fold 3 loss 0.041092194557496235\n",
      "Predicting results...\n",
      "153164/153164 [==============================] - 18s 117us/step\n",
      "Fold 4\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/4\n",
      "143614/143614 [==============================] - 128s 891us/step - loss: 0.1474 - acc: 0.9623 - val_loss: 0.0420 - val_acc: 0.9842\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04200, saving model to attngru_pp_fold4.h5\n",
      "Epoch 2/4\n",
      "143614/143614 [==============================] - 125s 872us/step - loss: 0.0410 - acc: 0.9841 - val_loss: 0.0403 - val_acc: 0.9848\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04200 to 0.04030, saving model to attngru_pp_fold4.h5\n",
      "Epoch 3/4\n",
      "143614/143614 [==============================] - 125s 871us/step - loss: 0.0363 - acc: 0.9855 - val_loss: 0.0406 - val_acc: 0.9847\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Fold 4 loss 0.04030366231099461\n",
      "Predicting results...\n",
      "153164/153164 [==============================] - 18s 117us/step\n",
      "Fold 5\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/4\n",
      "143614/143614 [==============================] - 127s 886us/step - loss: 0.1293 - acc: 0.9668 - val_loss: 0.0418 - val_acc: 0.9842\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04179, saving model to attngru_pp_fold5.h5\n",
      "Epoch 2/4\n",
      "143614/143614 [==============================] - 125s 870us/step - loss: 0.0417 - acc: 0.9838 - val_loss: 0.0392 - val_acc: 0.9845\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04179 to 0.03917, saving model to attngru_pp_fold5.h5\n",
      "Epoch 3/4\n",
      "143614/143614 [==============================] - 125s 873us/step - loss: 0.0366 - acc: 0.9856 - val_loss: 0.0385 - val_acc: 0.9851\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.03917 to 0.03851, saving model to attngru_pp_fold5.h5\n",
      "Epoch 4/4\n",
      "143614/143614 [==============================] - 125s 873us/step - loss: 0.0330 - acc: 0.9868 - val_loss: 0.0401 - val_acc: 0.9842\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Fold 5 loss 0.038506204056349094\n",
      "Predicting results...\n",
      "153164/153164 [==============================] - 18s 118us/step\n",
      "Fold 6\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/4\n",
      "143614/143614 [==============================] - 128s 889us/step - loss: 0.0987 - acc: 0.9707 - val_loss: 0.0424 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04236, saving model to attngru_pp_fold6.h5\n",
      "Epoch 2/4\n",
      "143614/143614 [==============================] - 126s 874us/step - loss: 0.0415 - acc: 0.9840 - val_loss: 0.0404 - val_acc: 0.9842\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04236 to 0.04039, saving model to attngru_pp_fold6.h5\n",
      "Epoch 3/4\n",
      "143614/143614 [==============================] - 126s 874us/step - loss: 0.0368 - acc: 0.9855 - val_loss: 0.0394 - val_acc: 0.9846\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04039 to 0.03943, saving model to attngru_pp_fold6.h5\n",
      "Epoch 4/4\n",
      "143614/143614 [==============================] - 125s 873us/step - loss: 0.0330 - acc: 0.9868 - val_loss: 0.0418 - val_acc: 0.9837\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Fold 6 loss 0.039433862140149246\n",
      "Predicting results...\n",
      "153164/153164 [==============================] - 18s 118us/step\n",
      "Fold 7\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/4\n",
      "143614/143614 [==============================] - 127s 886us/step - loss: 0.0998 - acc: 0.9720 - val_loss: 0.0432 - val_acc: 0.9834\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04319, saving model to attngru_pp_fold7.h5\n",
      "Epoch 2/4\n",
      "143614/143614 [==============================] - 125s 870us/step - loss: 0.0409 - acc: 0.9841 - val_loss: 0.0412 - val_acc: 0.9834\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04319 to 0.04121, saving model to attngru_pp_fold7.h5\n",
      "Epoch 3/4\n",
      "143614/143614 [==============================] - 125s 870us/step - loss: 0.0363 - acc: 0.9856 - val_loss: 0.0408 - val_acc: 0.9841\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04121 to 0.04075, saving model to attngru_pp_fold7.h5\n",
      "Epoch 4/4\n",
      "143614/143614 [==============================] - 125s 870us/step - loss: 0.0327 - acc: 0.9869 - val_loss: 0.0429 - val_acc: 0.9835\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Fold 7 loss 0.04075027770578334\n",
      "Predicting results...\n",
      "153164/153164 [==============================] - 18s 119us/step\n",
      "Fold 8\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/4\n",
      "143614/143614 [==============================] - 128s 889us/step - loss: 0.1253 - acc: 0.9622 - val_loss: 0.0412 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04125, saving model to attngru_pp_fold8.h5\n",
      "Epoch 2/4\n",
      "143614/143614 [==============================] - 125s 873us/step - loss: 0.0411 - acc: 0.9839 - val_loss: 0.0398 - val_acc: 0.9844\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04125 to 0.03976, saving model to attngru_pp_fold8.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/4\n",
      "143614/143614 [==============================] - 125s 870us/step - loss: 0.0368 - acc: 0.9855 - val_loss: 0.0385 - val_acc: 0.9852\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.03976 to 0.03855, saving model to attngru_pp_fold8.h5\n",
      "Epoch 4/4\n",
      "143614/143614 [==============================] - 125s 871us/step - loss: 0.0330 - acc: 0.9870 - val_loss: 0.0408 - val_acc: 0.9842\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Fold 8 loss 0.03854906819518003\n",
      "Predicting results...\n",
      "153164/153164 [==============================] - 18s 119us/step\n",
      "Fold 9\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/4\n",
      "143614/143614 [==============================] - 128s 892us/step - loss: 0.1095 - acc: 0.9684 - val_loss: 0.0426 - val_acc: 0.9839\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04262, saving model to attngru_pp_fold9.h5\n",
      "Epoch 2/4\n",
      "143614/143614 [==============================] - 125s 874us/step - loss: 0.0410 - acc: 0.9841 - val_loss: 0.0409 - val_acc: 0.9837\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04262 to 0.04091, saving model to attngru_pp_fold9.h5\n",
      "Epoch 3/4\n",
      "143614/143614 [==============================] - 126s 874us/step - loss: 0.0365 - acc: 0.9855 - val_loss: 0.0409 - val_acc: 0.9842\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04091 to 0.04087, saving model to attngru_pp_fold9.h5\n",
      "Epoch 4/4\n",
      "143614/143614 [==============================] - 125s 873us/step - loss: 0.0327 - acc: 0.9870 - val_loss: 0.0422 - val_acc: 0.9841\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Fold 9 loss 0.04087434388113969\n",
      "Predicting results...\n",
      "153164/153164 [==============================] - 18s 119us/step\n"
     ]
    }
   ],
   "source": [
    "train_folds(data, data_post, train_df[cols_f].as_matrix(), y, 10, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'CLASSES' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-523baf955150>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtest_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtest_predicts_am\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_predicts_am\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCLASSES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mtest_predicts_am\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtest_predicts_am\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_predicts_am\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mCLASSES\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CLASSES' is not defined"
     ]
    }
   ],
   "source": [
    "CLASSES = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "print(len(test_predicts_list))\n",
    "test_predicts_am = np.zeros(test_predicts_list[0].shape)\n",
    "\n",
    "for fold_predict in test_predicts_list:\n",
    "    test_predicts_am += fold_predict\n",
    "\n",
    "test_predicts_am = (test_predicts_am / len(test_predicts_list))\n",
    "\n",
    "test_ids = test_df[\"id\"].values\n",
    "test_ids = test_ids.reshape((len(test_ids), 1))\n",
    "\n",
    "test_predicts_am = pd.DataFrame(data=test_predicts_am, columns=CLASSES)\n",
    "test_predicts_am[\"id\"] = test_ids\n",
    "test_predicts_am = test_predicts_am[[\"id\"] + CLASSES]\n",
    "test_predicts_am.to_csv(\"10fold_gru_features_am.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicts = np.ones(test_predicts_list[0].shape)\n",
    "\n",
    "for fold_predict in test_predicts_list:\n",
    "    test_predicts *= fold_predict\n",
    "\n",
    "test_predicts **= (1. / len(test_predicts_list))\n",
    "\n",
    "test_ids = test_df[\"id\"].values\n",
    "test_ids = test_ids.reshape((len(test_ids), 1))\n",
    "\n",
    "test_predicts = pd.DataFrame(data=test_predicts, columns=CLASSES)\n",
    "test_predicts[\"id\"] = test_ids\n",
    "test_predicts = test_predicts[[\"id\"] + CLASSES]\n",
    "test_predicts.to_csv(\"10fold_attngru_gm.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>0.998139</td>\n",
       "      <td>0.501673</td>\n",
       "      <td>0.984929</td>\n",
       "      <td>0.114735</td>\n",
       "      <td>0.929604</td>\n",
       "      <td>0.395179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>0.000352</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>0.002678</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.000833</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>0.000830</td>\n",
       "      <td>0.000293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>0.006076</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000653</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.000575</td>\n",
       "      <td>0.000066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
       "0  00001cee341fdb12  0.998139      0.501673  0.984929  0.114735  0.929604   \n",
       "1  0000247867823ef7  0.000352      0.000021  0.000128  0.000015  0.000160   \n",
       "2  00013b17ad220c46  0.002678      0.000228  0.000833  0.000190  0.000830   \n",
       "3  00017563c3f7919a  0.000443      0.000022  0.000184  0.000055  0.000241   \n",
       "4  00017695ad8997eb  0.006076      0.000134  0.000653  0.000167  0.000575   \n",
       "\n",
       "   identity_hate  \n",
       "0       0.395179  \n",
       "1       0.000022  \n",
       "2       0.000293  \n",
       "3       0.000013  \n",
       "4       0.000066  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "o = pd.read_csv('10fold_gru_features_am.csv')\n",
    "o.to_csv('10fold_gru_features_am.csv', index=False)\n",
    "o.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
