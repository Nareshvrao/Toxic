{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: keras in /usr/local/lib/python3.6/dist-packages\n",
      "Requirement already up-to-date: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras)\n",
      "Requirement already up-to-date: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras)\n",
      "Requirement already up-to-date: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras)\n",
      "Requirement already up-to-date: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import *\n",
    "from keras.layers import concatenate, CuDNNGRU\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "        \n",
    "# path = '..'\n",
    "# EMBEDDING_FILE=path+'/input/glove.840B.300d.txt'\n",
    "# EMBEDDING_FILE=path+'/input/crawl-300d-2M.vec'\n",
    "# TRAIN_DATA_FILE=path+'/input/train.csv'\n",
    "# TEST_DATA_FILE=path+'/input/test.csv'\n",
    "\n",
    "EMBEDDING_FILE='/public/models/glove/glove.840B.300d.txt'\n",
    "TRAIN_DATA_FILE='/public/toxic_comments/train.csv'\n",
    "TEST_DATA_FILE='/public/toxic_comments/test.csv'\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "MAX_NB_WORDS = 100000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "num_lstm = 300\n",
    "num_dense = 256\n",
    "rate_drop_lstm = 0.2\n",
    "rate_drop_dense = 0.2\n",
    "\n",
    "act = 'relu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanData(text, stemming = False, lemmatize=False):    \n",
    "    text = text.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+\\-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    if stemming:\n",
    "        st = PorterStemmer()\n",
    "        txt = \" \".join([st.stem(w) for w in text.split()])\n",
    "    if lemmatize:\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        txt = \" \".join([wordnet_lemmatizer.lemmatize(w) for w in text.split()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors\n"
     ]
    }
   ],
   "source": [
    "print('Indexing word vectors')\n",
    "\n",
    "count = 0\n",
    "embeddings_index = {}\n",
    "f = open(EMBEDDING_FILE)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = ' '.join(values[:-300])\n",
    "    coefs = np.asarray(values[-300:], dtype='float32')\n",
    "    embeddings_index[word] = coefs.reshape(-1)\n",
    "    coef = embeddings_index[word]\n",
    "f.close()\n",
    "\n",
    "print('Found %d word vectors of glove.' % len(embeddings_index))\n",
    "emb_mean,emb_std = coef.mean(), coef.std()\n",
    "print(emb_mean,emb_std)\n",
    "\n",
    "print('Total %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test_df = pd.read_csv(TEST_DATA_FILE)\n",
    "\n",
    "print('Processing text dataset')\n",
    "\n",
    "train_df['comment_text'] = train_df['comment_text'].map(lambda x: cleanData(x,  stemming = False, \n",
    "                                                                            lemmatize=False))\n",
    "test_df['comment_text'] = test_df['comment_text'].map(lambda x: cleanData(x,  stemming = False, \n",
    "                                                                          lemmatize=False))\n",
    "\n",
    "#Regex to remove all Non-Alpha Numeric and space\n",
    "special_character_removal=re.compile(r'[^a-z\\d ]',re.IGNORECASE)\n",
    "#regex to replace all numerics\n",
    "replace_numbers=re.compile(r'\\d+',re.IGNORECASE)\n",
    "\n",
    "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    #Remove Special Characters\n",
    "    text=special_character_removal.sub('',text)\n",
    "    #Replace Numbers\n",
    "    text=replace_numbers.sub('n',text)\n",
    "\n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    return(text)\n",
    "\n",
    "\n",
    "list_sentences_train = train_df[\"comment_text\"].fillna(\"NA\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train_df[list_classes].values\n",
    "list_sentences_test = test_df[\"comment_text\"].fillna(\"NA\").values\n",
    "\n",
    "\n",
    "comments = []\n",
    "for text in list_sentences_train:\n",
    "    comments.append(text_to_wordlist(text))\n",
    "    \n",
    "test_comments=[]\n",
    "for text in list_sentences_test:\n",
    "    test_comments.append(text_to_wordlist(text))\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(comments + test_comments)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(comments)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_comments)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', y.shape)\n",
    "\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of test_data tensor:', test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_post = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH,padding='post', truncating='post')\n",
    "print('Shape of data tensor:', data_post.shape)\n",
    "print('Shape of label tensor:', y.shape)\n",
    "\n",
    "test_data_post = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "print('Shape of test_data tensor:', test_data_post.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Preparing embedding matrix')\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from os.path import dirname\n",
    "from keras import initializers\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "class AttentionWeightedAverage(Layer):\n",
    "    \"\"\"\n",
    "    Computes a weighted average of the different channels across timesteps.\n",
    "    Uses 1 parameter pr. channel to compute the attention value for a single timestep.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, return_attention=False, **kwargs):\n",
    "        self.init = initializers.get('uniform')\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        super(AttentionWeightedAverage, self).__init__(** kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(ndim=3)]\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[2], 1),\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 initializer=self.init)\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttentionWeightedAverage, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # computes a probability distribution over the timesteps\n",
    "        # uses 'max trick' for numerical stability\n",
    "        # reshape is done to avoid issue with Tensorflow\n",
    "        # and 1-dimensional weights\n",
    "        logits = K.dot(x, self.W)\n",
    "        x_shape = K.shape(x)\n",
    "        logits = K.reshape(logits, (x_shape[0], x_shape[1]))\n",
    "        ai = K.exp(logits - K.max(logits, axis=-1, keepdims=True))\n",
    "\n",
    "        # masked timesteps have zero weight\n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            ai = ai * mask\n",
    "        att_weights = ai / (K.sum(ai, axis=1, keepdims=True) + K.epsilon())\n",
    "        weighted_input = x * K.expand_dims(att_weights)\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "        if self.return_attention:\n",
    "            return [result, att_weights]\n",
    "        return result\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return self.compute_output_shape(input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_len = input_shape[2]\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]\n",
    "        return (input_shape[0], output_len)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        if isinstance(input_mask, list):\n",
    "            return [None] * len(input_mask)\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Nadam\n",
    "from keras.layers import concatenate\n",
    "\n",
    "def get_model():\n",
    "    filter_nr = 64\n",
    "    filter_size = 3\n",
    "    max_pool_size = 3\n",
    "    max_pool_strides = 2\n",
    "    dense_nr = 256\n",
    "    spatial_dropout = 0.4\n",
    "    dense_dropout = 0.4\n",
    "    \n",
    "    inp = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    inp_post = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "    emb_comment1 = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)(inp)\n",
    "    emb_comment1 = SpatialDropout1D(spatial_dropout)(emb_comment1)\n",
    "\n",
    "    block11 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(emb_comment1)\n",
    "    block11 = BatchNormalization()(block11)\n",
    "    block11 = PReLU()(block11)\n",
    "    block11 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block11)\n",
    "    block11 = BatchNormalization()(block11)\n",
    "    block11 = PReLU()(block11)\n",
    "\n",
    "    resize_emb1 = Conv1D(filter_nr, kernel_size=1, padding='same', activation='linear')(emb_comment1)\n",
    "    resize_emb1 = PReLU()(resize_emb1)\n",
    "\n",
    "    block1_output1 = add([block11, resize_emb1])\n",
    "    block1_output1 = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block1_output1)\n",
    "\n",
    "    block21 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block1_output1)\n",
    "    block21 = BatchNormalization()(block21)\n",
    "    block21 = PReLU()(block21)\n",
    "    block21 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block21)\n",
    "    block21 = BatchNormalization()(block21)\n",
    "    block21 = PReLU()(block21)\n",
    "\n",
    "    block2_output1 = add([block21, block1_output1])\n",
    "    block2_output1 = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block2_output1)\n",
    "\n",
    "    block31 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block2_output1)\n",
    "    block31 = BatchNormalization()(block31)\n",
    "    block31 = PReLU()(block31)\n",
    "    block31 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block31)\n",
    "    block31 = BatchNormalization()(block31)\n",
    "    block31 = PReLU()(block31)\n",
    "\n",
    "    block3_output1 = add([block31, block2_output1])\n",
    "    block3_output1 = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block3_output1)\n",
    "\n",
    "    block41 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block3_output1)\n",
    "    block41 = BatchNormalization()(block41)\n",
    "    block41 = PReLU()(block41)\n",
    "    block41 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block41)\n",
    "    block41 = BatchNormalization()(block41)\n",
    "    block41 = PReLU()(block41)\n",
    "\n",
    "    output1 = add([block41, block3_output1])\n",
    "    output1 = GlobalMaxPooling1D()(output1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    emb_comment2 = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix], trainable=False)(inp_post)\n",
    "    emb_comment2 = SpatialDropout1D(spatial_dropout)(emb_comment2)\n",
    "\n",
    "    block12 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(emb_comment2)\n",
    "    block12 = BatchNormalization()(block12)\n",
    "    block12 = PReLU()(block12)\n",
    "    block12 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block12)\n",
    "    block12 = BatchNormalization()(block12)\n",
    "    block12 = PReLU()(block12)\n",
    "\n",
    "    resize_emb2 = Conv1D(filter_nr, kernel_size=1, padding='same', activation='linear')(emb_comment2)\n",
    "    resize_emb2 = PReLU()(resize_emb2)\n",
    "\n",
    "    block1_output2 = add([block12, resize_emb2])\n",
    "    block1_output2 = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block1_output2)\n",
    "\n",
    "    block22 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block1_output2)\n",
    "    block22 = BatchNormalization()(block22)\n",
    "    block22 = PReLU()(block22)\n",
    "    block22 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block22)\n",
    "    block22 = BatchNormalization()(block22)\n",
    "    block22 = PReLU()(block22)\n",
    "\n",
    "    block2_output2 = add([block22, block1_output2])\n",
    "    block2_output2 = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block2_output2)\n",
    "\n",
    "    block32 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block2_output2)\n",
    "    block32 = BatchNormalization()(block32)\n",
    "    block32 = PReLU()(block32)\n",
    "    block32 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block32)\n",
    "    block32 = BatchNormalization()(block32)\n",
    "    block32 = PReLU()(block32)\n",
    "\n",
    "    block3_output2 = add([block32, block2_output2])\n",
    "    block3_output2 = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block3_output2)\n",
    "\n",
    "    block42 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block3_output2)\n",
    "    block42 = BatchNormalization()(block42)\n",
    "    block42 = PReLU()(block42)\n",
    "    block42 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(block42)\n",
    "    block42 = BatchNormalization()(block42)\n",
    "    block42 = PReLU()(block42)\n",
    "\n",
    "    output2 = add([block42, block3_output2])\n",
    "    output2 = GlobalMaxPooling1D()(output2)\n",
    "\n",
    "            \n",
    "    output = concatenate([output1, output2])\n",
    "    output = Dense(dense_nr, activation='linear')(output)\n",
    "    output = BatchNormalization()(output)\n",
    "    output = PReLU()(output)\n",
    "    output = Dropout(dense_dropout)(output)\n",
    "    output = Dense(6, activation='sigmoid')(output)\n",
    "   \n",
    "    model = Model(inputs=[inp, inp_post], outputs=output, name=\"DPCNN\")\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "        optimizer=Adam(lr=1e-3,decay=0),\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 200, 300)     30000000    input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 200, 300)     30000000    input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_7 (SpatialDro (None, 200, 300)     0           embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_8 (SpatialDro (None, 200, 300)     0           embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 200, 64)      57664       spatial_dropout1d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, 200, 64)      57664       spatial_dropout1d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 200, 64)      256         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 200, 64)      256         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_58 (PReLU)              (None, 200, 64)      12800       batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_67 (PReLU)              (None, 200, 64)      12800       batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 200, 64)      12352       p_re_lu_58[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 200, 64)      12352       p_re_lu_67[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 200, 64)      256         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 200, 64)      19264       spatial_dropout1d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 200, 64)      256         conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_66 (Conv1D)              (None, 200, 64)      19264       spatial_dropout1d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_59 (PReLU)              (None, 200, 64)      12800       batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_60 (PReLU)              (None, 200, 64)      12800       conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_68 (PReLU)              (None, 200, 64)      12800       batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_69 (PReLU)              (None, 200, 64)      12800       conv1d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 200, 64)      0           p_re_lu_59[0][0]                 \n",
      "                                                                 p_re_lu_60[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_29 (Add)                    (None, 200, 64)      0           p_re_lu_68[0][0]                 \n",
      "                                                                 p_re_lu_69[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 99, 64)       0           add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 99, 64)       0           add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 99, 64)       12352       max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_67 (Conv1D)              (None, 99, 64)       12352       max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 99, 64)       256         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 99, 64)       256         conv1d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_61 (PReLU)              (None, 99, 64)       6336        batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_70 (PReLU)              (None, 99, 64)       6336        batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 99, 64)       12352       p_re_lu_61[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_68 (Conv1D)              (None, 99, 64)       12352       p_re_lu_70[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 99, 64)       256         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 99, 64)       256         conv1d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_62 (PReLU)              (None, 99, 64)       6336        batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_71 (PReLU)              (None, 99, 64)       6336        batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 99, 64)       0           p_re_lu_62[0][0]                 \n",
      "                                                                 max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_30 (Add)                    (None, 99, 64)       0           p_re_lu_71[0][0]                 \n",
      "                                                                 max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 49, 64)       0           add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 49, 64)       0           add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 49, 64)       12352       max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_69 (Conv1D)              (None, 49, 64)       12352       max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 49, 64)       256         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 49, 64)       256         conv1d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_63 (PReLU)              (None, 49, 64)       3136        batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_72 (PReLU)              (None, 49, 64)       3136        batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 49, 64)       12352       p_re_lu_63[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_70 (Conv1D)              (None, 49, 64)       12352       p_re_lu_72[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 49, 64)       256         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 49, 64)       256         conv1d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_64 (PReLU)              (None, 49, 64)       3136        batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_73 (PReLU)              (None, 49, 64)       3136        batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_27 (Add)                    (None, 49, 64)       0           p_re_lu_64[0][0]                 \n",
      "                                                                 max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_31 (Add)                    (None, 49, 64)       0           p_re_lu_73[0][0]                 \n",
      "                                                                 max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling1D) (None, 24, 64)       0           add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 24, 64)       0           add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 24, 64)       12352       max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_71 (Conv1D)              (None, 24, 64)       12352       max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 24, 64)       256         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 24, 64)       256         conv1d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_65 (PReLU)              (None, 24, 64)       1536        batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_74 (PReLU)              (None, 24, 64)       1536        batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 24, 64)       12352       p_re_lu_65[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_72 (Conv1D)              (None, 24, 64)       12352       p_re_lu_74[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 24, 64)       256         conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 24, 64)       256         conv1d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_66 (PReLU)              (None, 24, 64)       1536        batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_75 (PReLU)              (None, 24, 64)       1536        batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_28 (Add)                    (None, 24, 64)       0           p_re_lu_66[0][0]                 \n",
      "                                                                 max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_32 (Add)                    (None, 24, 64)       0           p_re_lu_75[0][0]                 \n",
      "                                                                 max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 64)           0           add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 64)           0           add_32[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 128)          0           global_max_pooling1d_7[0][0]     \n",
      "                                                                 global_max_pooling1d_8[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          33024       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 256)          1024        dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_76 (PReLU)              (None, 256)          256         batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 256)          0           p_re_lu_76[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 6)            1542        dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 60,487,558\n",
      "Trainable params: 484,998\n",
      "Non-trainable params: 60,002,560\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "import numpy as np\n",
    "\n",
    "test_predicts_list = []\n",
    "\n",
    "def train_folds(data,data_post, y, fold_count, batch_size):\n",
    "    print(\"Starting to train models...\")\n",
    "    fold_size = len(data) // fold_count\n",
    "    models = []\n",
    "    for fold_id in range(0, fold_count):\n",
    "        fold_start = fold_size * fold_id\n",
    "        fold_end = fold_start + fold_size\n",
    "\n",
    "        if fold_id == fold_size - 1:\n",
    "            fold_end = len(data)\n",
    "\n",
    "        print(\"Fold {0}\".format(fold_id))\n",
    "        \n",
    "        train_x = np.concatenate([data[:fold_start], data[fold_end:]])\n",
    "        train_xp = np.concatenate([data_post[:fold_start], data_post[fold_end:]])\n",
    "        train_y = np.concatenate([y[:fold_start], y[fold_end:]])\n",
    "\n",
    "        val_x = data[fold_start:fold_end]\n",
    "        val_xp = data_post[fold_start:fold_end]\n",
    "        val_y = y[fold_start:fold_end]\n",
    "        \n",
    "        file_path=\"dpcnn_fold{0}.h5\".format(fold_id)\n",
    "        model = get_model()\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3)\n",
    "        callbacks_list = [checkpoint, early] \n",
    "\n",
    "        hist = model.fit([train_x, train_xp], train_y, epochs=15, batch_size=128, shuffle=True, \n",
    "                         validation_data=([val_x, val_xp], val_y), callbacks = callbacks_list, verbose=1)\n",
    "        model.load_weights(file_path)\n",
    "        best_score = min(hist.history['val_loss'])\n",
    "        \n",
    "        print(\"Fold {0} loss {1}\".format(fold_id, best_score))\n",
    "        print(\"Predicting validation...\")\n",
    "        val_predicts_path = \"dpcnn_val_predicts{0}.npy\".format(fold_id)\n",
    "        val_predicts = model.predict([val_x, val_xp], batch_size=1024, verbose=1)\n",
    "        np.save(val_predicts_path, val_predicts)\n",
    "        \n",
    "        print(\"Predicting results...\")\n",
    "        test_predicts_path = \"dpcnn_test_predicts{0}.npy\".format(fold_id)\n",
    "        test_predicts = model.predict([test_data, test_data_post], batch_size=1024, verbose=1)\n",
    "        test_predicts_list.append(test_predicts)\n",
    "        np.save(test_predicts_path, test_predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train models...\n",
      "Fold 0\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/15\n",
      "143614/143614 [==============================] - 126s 875us/step - loss: 0.0671 - acc: 0.9764 - val_loss: 0.0461 - val_acc: 0.9823\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04606, saving model to dpcnn_fold0.h5\n",
      "Epoch 2/15\n",
      "143614/143614 [==============================] - 121s 842us/step - loss: 0.0475 - acc: 0.9820 - val_loss: 0.0499 - val_acc: 0.9815\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/15\n",
      "143614/143614 [==============================] - 121s 840us/step - loss: 0.0442 - acc: 0.9828 - val_loss: 0.0427 - val_acc: 0.9831\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04606 to 0.04274, saving model to dpcnn_fold0.h5\n",
      "Epoch 4/15\n",
      "143614/143614 [==============================] - 121s 846us/step - loss: 0.0420 - acc: 0.9835 - val_loss: 0.0408 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04274 to 0.04081, saving model to dpcnn_fold0.h5\n",
      "Epoch 5/15\n",
      "143614/143614 [==============================] - 122s 846us/step - loss: 0.0403 - acc: 0.9840 - val_loss: 0.0436 - val_acc: 0.9830\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/15\n",
      "143614/143614 [==============================] - 121s 843us/step - loss: 0.0389 - acc: 0.9844 - val_loss: 0.0418 - val_acc: 0.9832\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/15\n",
      "143614/143614 [==============================] - 122s 847us/step - loss: 0.0377 - acc: 0.9848 - val_loss: 0.0393 - val_acc: 0.9846\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.04081 to 0.03934, saving model to dpcnn_fold0.h5\n",
      "Epoch 8/15\n",
      "143614/143614 [==============================] - 121s 843us/step - loss: 0.0363 - acc: 0.9853 - val_loss: 0.0395 - val_acc: 0.9841\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/15\n",
      "143614/143614 [==============================] - 121s 840us/step - loss: 0.0349 - acc: 0.9859 - val_loss: 0.0412 - val_acc: 0.9842\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/15\n",
      "143614/143614 [==============================] - 120s 835us/step - loss: 0.0338 - acc: 0.9862 - val_loss: 0.0409 - val_acc: 0.9845\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Fold 0 loss 0.03933982255913496\n",
      "Predicting validation...\n",
      "15957/15957 [==============================] - 4s 274us/step\n",
      "Predicting results...\n",
      "153164/153164 [==============================] - 31s 201us/step\n",
      "Fold 1\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/15\n",
      "143614/143614 [==============================] - 127s 881us/step - loss: 0.0695 - acc: 0.9752 - val_loss: 0.0492 - val_acc: 0.9811\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04922, saving model to dpcnn_fold1.h5\n",
      "Epoch 2/15\n",
      "143614/143614 [==============================] - 121s 843us/step - loss: 0.0473 - acc: 0.9821 - val_loss: 0.0449 - val_acc: 0.9829\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04922 to 0.04486, saving model to dpcnn_fold1.h5\n",
      "Epoch 3/15\n",
      "143614/143614 [==============================] - 121s 843us/step - loss: 0.0439 - acc: 0.9829 - val_loss: 0.0443 - val_acc: 0.9825\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04486 to 0.04434, saving model to dpcnn_fold1.h5\n",
      "Epoch 4/15\n",
      "143614/143614 [==============================] - 122s 849us/step - loss: 0.0420 - acc: 0.9835 - val_loss: 0.0434 - val_acc: 0.9827\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04434 to 0.04341, saving model to dpcnn_fold1.h5\n",
      "Epoch 5/15\n",
      "143614/143614 [==============================] - 122s 851us/step - loss: 0.0402 - acc: 0.9840 - val_loss: 0.0424 - val_acc: 0.9833\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.04341 to 0.04242, saving model to dpcnn_fold1.h5\n",
      "Epoch 6/15\n",
      "143614/143614 [==============================] - 122s 846us/step - loss: 0.0391 - acc: 0.9844 - val_loss: 0.0409 - val_acc: 0.9834\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.04242 to 0.04093, saving model to dpcnn_fold1.h5\n",
      "Epoch 7/15\n",
      "143614/143614 [==============================] - 122s 849us/step - loss: 0.0373 - acc: 0.9849 - val_loss: 0.0427 - val_acc: 0.9831\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/15\n",
      "143614/143614 [==============================] - 122s 847us/step - loss: 0.0362 - acc: 0.9854 - val_loss: 0.0437 - val_acc: 0.9833\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/15\n",
      "143614/143614 [==============================] - 121s 846us/step - loss: 0.0348 - acc: 0.9858 - val_loss: 0.0421 - val_acc: 0.9831\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Fold 1 loss 0.04093192515049302\n",
      "Predicting validation...\n",
      "15957/15957 [==============================] - 5s 296us/step\n",
      "Predicting results...\n",
      "153164/153164 [==============================] - 31s 202us/step\n",
      "Fold 2\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/15\n",
      "143614/143614 [==============================] - 130s 902us/step - loss: 0.0685 - acc: 0.9758 - val_loss: 0.0530 - val_acc: 0.9797\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05299, saving model to dpcnn_fold2.h5\n",
      "Epoch 2/15\n",
      "143614/143614 [==============================] - 123s 855us/step - loss: 0.0476 - acc: 0.9820 - val_loss: 0.0416 - val_acc: 0.9838\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05299 to 0.04161, saving model to dpcnn_fold2.h5\n",
      "Epoch 3/15\n",
      "143614/143614 [==============================] - 124s 862us/step - loss: 0.0442 - acc: 0.9829 - val_loss: 0.0403 - val_acc: 0.9838\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04161 to 0.04029, saving model to dpcnn_fold2.h5\n",
      "Epoch 4/15\n",
      "143614/143614 [==============================] - 124s 865us/step - loss: 0.0422 - acc: 0.9835 - val_loss: 0.0399 - val_acc: 0.9839\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04029 to 0.03990, saving model to dpcnn_fold2.h5\n",
      "Epoch 5/15\n",
      "143614/143614 [==============================] - 124s 862us/step - loss: 0.0404 - acc: 0.9841 - val_loss: 0.0408 - val_acc: 0.9835\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/15\n",
      "143614/143614 [==============================] - 123s 858us/step - loss: 0.0391 - acc: 0.9844 - val_loss: 0.0394 - val_acc: 0.9844\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.03990 to 0.03938, saving model to dpcnn_fold2.h5\n",
      "Epoch 7/15\n",
      "143614/143614 [==============================] - 124s 863us/step - loss: 0.0377 - acc: 0.9850 - val_loss: 0.0391 - val_acc: 0.9842\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.03938 to 0.03906, saving model to dpcnn_fold2.h5\n",
      "Epoch 8/15\n",
      "143614/143614 [==============================] - 124s 863us/step - loss: 0.0365 - acc: 0.9852 - val_loss: 0.0387 - val_acc: 0.9845\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.03906 to 0.03868, saving model to dpcnn_fold2.h5\n",
      "Epoch 9/15\n",
      "143614/143614 [==============================] - 124s 860us/step - loss: 0.0350 - acc: 0.9858 - val_loss: 0.0389 - val_acc: 0.9847\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/15\n",
      "143614/143614 [==============================] - 124s 861us/step - loss: 0.0339 - acc: 0.9860 - val_loss: 0.0400 - val_acc: 0.9842\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/15\n",
      "143614/143614 [==============================] - 124s 861us/step - loss: 0.0327 - acc: 0.9864 - val_loss: 0.0415 - val_acc: 0.9830\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Fold 2 loss 0.038680556249915864\n",
      "Predicting validation...\n",
      "15957/15957 [==============================] - 5s 342us/step\n",
      "Predicting results...\n",
      "153164/153164 [==============================] - 31s 202us/step\n",
      "Fold 3\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/15\n",
      "143614/143614 [==============================] - 130s 903us/step - loss: 0.0729 - acc: 0.9732 - val_loss: 0.0489 - val_acc: 0.9817\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04891, saving model to dpcnn_fold3.h5\n",
      "Epoch 2/15\n",
      "143614/143614 [==============================] - 123s 854us/step - loss: 0.0476 - acc: 0.9821 - val_loss: 0.0456 - val_acc: 0.9828\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04891 to 0.04558, saving model to dpcnn_fold3.h5\n",
      "Epoch 3/15\n",
      "143614/143614 [==============================] - 123s 859us/step - loss: 0.0442 - acc: 0.9829 - val_loss: 0.0435 - val_acc: 0.9834\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04558 to 0.04349, saving model to dpcnn_fold3.h5\n",
      "Epoch 4/15\n",
      "143614/143614 [==============================] - 123s 857us/step - loss: 0.0419 - acc: 0.9836 - val_loss: 0.0419 - val_acc: 0.9841\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04349 to 0.04193, saving model to dpcnn_fold3.h5\n",
      "Epoch 5/15\n",
      "143614/143614 [==============================] - 123s 855us/step - loss: 0.0402 - acc: 0.9841 - val_loss: 0.0406 - val_acc: 0.9842\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.04193 to 0.04055, saving model to dpcnn_fold3.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15\n",
      "143614/143614 [==============================] - 123s 858us/step - loss: 0.0389 - acc: 0.9844 - val_loss: 0.0424 - val_acc: 0.9842\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/15\n",
      "143614/143614 [==============================] - 123s 860us/step - loss: 0.0376 - acc: 0.9848 - val_loss: 0.0426 - val_acc: 0.9843\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/15\n",
      "143614/143614 [==============================] - 123s 859us/step - loss: 0.0363 - acc: 0.9853 - val_loss: 0.0419 - val_acc: 0.9839\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Fold 3 loss 0.040550566945913004\n",
      "Predicting validation...\n",
      "15957/15957 [==============================] - 6s 373us/step\n",
      "Predicting results...\n",
      "153164/153164 [==============================] - 31s 202us/step\n",
      "Fold 4\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/15\n",
      "143614/143614 [==============================] - 133s 923us/step - loss: 0.0673 - acc: 0.9761 - val_loss: 0.0459 - val_acc: 0.9823\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04592, saving model to dpcnn_fold4.h5\n",
      "Epoch 2/15\n",
      "143614/143614 [==============================] - 124s 864us/step - loss: 0.0475 - acc: 0.9820 - val_loss: 0.0426 - val_acc: 0.9834\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04592 to 0.04257, saving model to dpcnn_fold4.h5\n",
      "Epoch 3/15\n",
      "143614/143614 [==============================] - 122s 853us/step - loss: 0.0442 - acc: 0.9829 - val_loss: 0.0446 - val_acc: 0.9829\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/15\n",
      "143614/143614 [==============================] - 123s 854us/step - loss: 0.0422 - acc: 0.9834 - val_loss: 0.0400 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04257 to 0.03995, saving model to dpcnn_fold4.h5\n",
      "Epoch 5/15\n",
      "143614/143614 [==============================] - 123s 857us/step - loss: 0.0404 - acc: 0.9840 - val_loss: 0.0412 - val_acc: 0.9843\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/15\n",
      "143614/143614 [==============================] - 122s 853us/step - loss: 0.0389 - acc: 0.9845 - val_loss: 0.0396 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.03995 to 0.03959, saving model to dpcnn_fold4.h5\n",
      "Epoch 7/15\n",
      "143614/143614 [==============================] - 123s 855us/step - loss: 0.0376 - acc: 0.9849 - val_loss: 0.0410 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/15\n",
      "143614/143614 [==============================] - 124s 864us/step - loss: 0.0363 - acc: 0.9854 - val_loss: 0.0402 - val_acc: 0.9846\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/15\n",
      "143614/143614 [==============================] - 125s 868us/step - loss: 0.0351 - acc: 0.9856 - val_loss: 0.0407 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Fold 4 loss 0.03958821773203145\n",
      "Predicting validation...\n",
      "15957/15957 [==============================] - 6s 367us/step\n",
      "Predicting results...\n",
      "153164/153164 [==============================] - 31s 202us/step\n",
      "Fold 5\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/15\n",
      "143614/143614 [==============================] - 136s 949us/step - loss: 0.0671 - acc: 0.9764 - val_loss: 0.0473 - val_acc: 0.9817\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04735, saving model to dpcnn_fold5.h5\n",
      "Epoch 2/15\n",
      "143614/143614 [==============================] - 127s 883us/step - loss: 0.0478 - acc: 0.9819 - val_loss: 0.0414 - val_acc: 0.9837\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04735 to 0.04141, saving model to dpcnn_fold5.h5\n",
      "Epoch 3/15\n",
      "143614/143614 [==============================] - 127s 883us/step - loss: 0.0442 - acc: 0.9829 - val_loss: 0.0405 - val_acc: 0.9841\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04141 to 0.04054, saving model to dpcnn_fold5.h5\n",
      "Epoch 4/15\n",
      "143614/143614 [==============================] - 127s 885us/step - loss: 0.0423 - acc: 0.9832 - val_loss: 0.0389 - val_acc: 0.9849\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04054 to 0.03886, saving model to dpcnn_fold5.h5\n",
      "Epoch 5/15\n",
      "143614/143614 [==============================] - 127s 884us/step - loss: 0.0408 - acc: 0.9839 - val_loss: 0.0384 - val_acc: 0.9847\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.03886 to 0.03844, saving model to dpcnn_fold5.h5\n",
      "Epoch 6/15\n",
      "143614/143614 [==============================] - 125s 867us/step - loss: 0.0393 - acc: 0.9843 - val_loss: 0.0404 - val_acc: 0.9846\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/15\n",
      "143614/143614 [==============================] - 125s 870us/step - loss: 0.0381 - acc: 0.9848 - val_loss: 0.0389 - val_acc: 0.9847\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/15\n",
      "143614/143614 [==============================] - 124s 863us/step - loss: 0.0366 - acc: 0.9850 - val_loss: 0.0392 - val_acc: 0.9849\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Fold 5 loss 0.03843847780013403\n",
      "Predicting validation...\n",
      "15957/15957 [==============================] - 6s 378us/step\n",
      "Predicting results...\n",
      "153164/153164 [==============================] - 31s 202us/step\n",
      "Fold 6\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/15\n",
      "143614/143614 [==============================] - 131s 911us/step - loss: 0.0676 - acc: 0.9762 - val_loss: 0.0532 - val_acc: 0.9799\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05325, saving model to dpcnn_fold6.h5\n",
      "Epoch 2/15\n",
      "143614/143614 [==============================] - 122s 853us/step - loss: 0.0480 - acc: 0.9819 - val_loss: 0.0417 - val_acc: 0.9838\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05325 to 0.04168, saving model to dpcnn_fold6.h5\n",
      "Epoch 3/15\n",
      "143614/143614 [==============================] - 122s 848us/step - loss: 0.0443 - acc: 0.9829 - val_loss: 0.0419 - val_acc: 0.9839\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/15\n",
      "143614/143614 [==============================] - 122s 849us/step - loss: 0.0422 - acc: 0.9835 - val_loss: 0.0438 - val_acc: 0.9825\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/15\n",
      "143614/143614 [==============================] - 122s 850us/step - loss: 0.0404 - acc: 0.9840 - val_loss: 0.0402 - val_acc: 0.9843\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.04168 to 0.04023, saving model to dpcnn_fold6.h5\n",
      "Epoch 6/15\n",
      "143614/143614 [==============================] - 122s 848us/step - loss: 0.0389 - acc: 0.9845 - val_loss: 0.0404 - val_acc: 0.9838\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/15\n",
      "143614/143614 [==============================] - 123s 856us/step - loss: 0.0376 - acc: 0.9849 - val_loss: 0.0437 - val_acc: 0.9830\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/15\n",
      "143614/143614 [==============================] - 123s 857us/step - loss: 0.0361 - acc: 0.9853 - val_loss: 0.0405 - val_acc: 0.9839\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Fold 6 loss 0.040225764995788345\n",
      "Predicting validation...\n",
      "15957/15957 [==============================] - 7s 441us/step\n",
      "Predicting results...\n",
      "153164/153164 [==============================] - 31s 202us/step\n",
      "Fold 7\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/15\n",
      "143614/143614 [==============================] - 133s 923us/step - loss: 0.0678 - acc: 0.9762 - val_loss: 0.0471 - val_acc: 0.9821\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04709, saving model to dpcnn_fold7.h5\n",
      "Epoch 2/15\n",
      "143614/143614 [==============================] - 124s 860us/step - loss: 0.0476 - acc: 0.9821 - val_loss: 0.0471 - val_acc: 0.9821\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/15\n",
      "143614/143614 [==============================] - 123s 856us/step - loss: 0.0444 - acc: 0.9829 - val_loss: 0.0433 - val_acc: 0.9827\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04709 to 0.04331, saving model to dpcnn_fold7.h5\n",
      "Epoch 4/15\n",
      "143614/143614 [==============================] - 124s 860us/step - loss: 0.0405 - acc: 0.9839 - val_loss: 0.0440 - val_acc: 0.9827\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/15\n",
      "143614/143614 [==============================] - 123s 858us/step - loss: 0.0392 - acc: 0.9844 - val_loss: 0.0417 - val_acc: 0.9835\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/15\n",
      "143614/143614 [==============================] - 123s 859us/step - loss: 0.0377 - acc: 0.9849 - val_loss: 0.0407 - val_acc: 0.9833\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.04076 to 0.04075, saving model to dpcnn_fold7.h5\n",
      "Epoch 8/15\n",
      "143614/143614 [==============================] - 123s 859us/step - loss: 0.0366 - acc: 0.9852 - val_loss: 0.0411 - val_acc: 0.9837\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/15\n",
      "143614/143614 [==============================] - 124s 863us/step - loss: 0.0352 - acc: 0.9856 - val_loss: 0.0406 - val_acc: 0.9836\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.04075 to 0.04062, saving model to dpcnn_fold7.h5\n",
      "Epoch 10/15\n",
      "143614/143614 [==============================] - 123s 858us/step - loss: 0.0341 - acc: 0.9861 - val_loss: 0.0424 - val_acc: 0.9832\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/15\n",
      "143614/143614 [==============================] - 124s 861us/step - loss: 0.0330 - acc: 0.9864 - val_loss: 0.0432 - val_acc: 0.9834\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/15\n",
      "143614/143614 [==============================] - 124s 861us/step - loss: 0.0316 - acc: 0.9868 - val_loss: 0.0427 - val_acc: 0.9836\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Fold 7 loss 0.04061930055265522\n",
      "Predicting validation...\n",
      "15957/15957 [==============================] - 7s 418us/step\n",
      "Predicting results...\n",
      "153164/153164 [==============================] - 31s 203us/step\n",
      "Fold 8\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/15\n",
      "143614/143614 [==============================] - 134s 933us/step - loss: 0.0663 - acc: 0.9766 - val_loss: 0.0497 - val_acc: 0.9815\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04970, saving model to dpcnn_fold8.h5\n",
      "Epoch 2/15\n",
      "143614/143614 [==============================] - 124s 863us/step - loss: 0.0475 - acc: 0.9820 - val_loss: 0.0433 - val_acc: 0.9836\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04970 to 0.04331, saving model to dpcnn_fold8.h5\n",
      "Epoch 3/15\n",
      "143614/143614 [==============================] - 124s 863us/step - loss: 0.0445 - acc: 0.9828 - val_loss: 0.0410 - val_acc: 0.9842\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04331 to 0.04103, saving model to dpcnn_fold8.h5\n",
      "Epoch 4/15\n",
      "143614/143614 [==============================] - 124s 862us/step - loss: 0.0422 - acc: 0.9834 - val_loss: 0.0408 - val_acc: 0.9844\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04103 to 0.04075, saving model to dpcnn_fold8.h5\n",
      "Epoch 5/15\n",
      "143614/143614 [==============================] - 124s 862us/step - loss: 0.0404 - acc: 0.9840 - val_loss: 0.0402 - val_acc: 0.9846\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.04075 to 0.04022, saving model to dpcnn_fold8.h5\n",
      "Epoch 6/15\n",
      "143614/143614 [==============================] - 123s 860us/step - loss: 0.0389 - acc: 0.9845 - val_loss: 0.0448 - val_acc: 0.9839\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/15\n",
      "143614/143614 [==============================] - 124s 862us/step - loss: 0.0379 - acc: 0.9847 - val_loss: 0.0406 - val_acc: 0.9841\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/15\n",
      "143614/143614 [==============================] - 124s 864us/step - loss: 0.0364 - acc: 0.9852 - val_loss: 0.0414 - val_acc: 0.9838\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Fold 8 loss 0.040216939961433246\n",
      "Predicting validation...\n",
      "15957/15957 [==============================] - 7s 446us/step\n",
      "Predicting results...\n",
      "153164/153164 [==============================] - 31s 203us/step\n",
      "Fold 9\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/15\n",
      "143614/143614 [==============================] - 135s 940us/step - loss: 0.0690 - acc: 0.9755 - val_loss: 0.0522 - val_acc: 0.9814\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05218, saving model to dpcnn_fold9.h5\n",
      "Epoch 2/15\n",
      "143614/143614 [==============================] - 124s 865us/step - loss: 0.0474 - acc: 0.9821 - val_loss: 0.0470 - val_acc: 0.9818\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05218 to 0.04699, saving model to dpcnn_fold9.h5\n",
      "Epoch 3/15\n",
      "143614/143614 [==============================] - 124s 864us/step - loss: 0.0443 - acc: 0.9829 - val_loss: 0.0426 - val_acc: 0.9832\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04699 to 0.04255, saving model to dpcnn_fold9.h5\n",
      "Epoch 4/15\n",
      "142720/143614 [============================>.] - ETA: 0s - loss: 0.0422 - acc: 0.9835"
     ]
    }
   ],
   "source": [
    "train_folds(data, data_post, y, 10, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "print(len(test_predicts_list))\n",
    "test_predicts_am = np.zeros(test_predicts_list[0].shape)\n",
    "\n",
    "for fold_predict in test_predicts_list:\n",
    "    test_predicts_am += fold_predict\n",
    "\n",
    "test_predicts_am = (test_predicts_am / len(test_predicts_list))\n",
    "\n",
    "test_ids = test_df[\"id\"].values\n",
    "test_ids = test_ids.reshape((len(test_ids), 1))\n",
    "\n",
    "test_predicts_am = pd.DataFrame(data=test_predicts_am, columns=CLASSES)\n",
    "test_predicts_am[\"id\"] = test_ids\n",
    "test_predicts_am = test_predicts_am[[\"id\"] + CLASSES]\n",
    "test_predicts_am.to_csv(\"10fold_dpcnn_am.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicts = np.ones(test_predicts_list[0].shape)\n",
    "\n",
    "for fold_predict in test_predicts_list:\n",
    "    test_predicts *= fold_predict\n",
    "\n",
    "test_predicts **= (1. / len(test_predicts_list))\n",
    "\n",
    "test_ids = test_df[\"id\"].values\n",
    "test_ids = test_ids.reshape((len(test_ids), 1))\n",
    "\n",
    "test_predicts = pd.DataFrame(data=test_predicts, columns=CLASSES)\n",
    "test_predicts[\"id\"] = test_ids\n",
    "test_predicts = test_predicts[[\"id\"] + CLASSES]\n",
    "test_predicts.to_csv(\"10fold_dpcnn_gm.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
