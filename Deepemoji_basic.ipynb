{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading Keras-2.1.5-py2.py3-none-any.whl (334kB)\n",
      "\u001b[K    100% |████████████████████████████████| 337kB 3.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyyaml (from keras)\n",
      "Collecting six>=1.9.0 (from keras)\n",
      "  Using cached six-1.11.0-py2.py3-none-any.whl\n",
      "Requirement already up-to-date: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras)\n",
      "Collecting numpy>=1.9.1 (from keras)\n",
      "  Downloading numpy-1.14.2-cp36-cp36m-manylinux1_x86_64.whl (12.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 12.2MB 126kB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pyyaml, six, numpy, keras\n",
      "  Found existing installation: PyYAML 3.11\n",
      "    Uninstalling PyYAML-3.11:\n",
      "      Successfully uninstalled PyYAML-3.11\n",
      "  Found existing installation: six 1.10.0\n",
      "    Uninstalling six-1.10.0:\n",
      "      Successfully uninstalled six-1.10.0\n",
      "  Found existing installation: numpy 1.14.0\n",
      "    Uninstalling numpy-1.14.0:\n",
      "      Successfully uninstalled numpy-1.14.0\n",
      "  Found existing installation: Keras 2.0.6\n",
      "    Uninstalling Keras-2.0.6:\n",
      "      Successfully uninstalled Keras-2.0.6\n",
      "Successfully installed keras-2.1.5 numpy-1.14.2 pyyaml-3.12 six-1.11.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import *\n",
    "from keras.layers import concatenate, CuDNNGRU\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "        \n",
    "# path = '..'\n",
    "# EMBEDDING_FILE=path+'/input/glove.840B.300d.txt'\n",
    "# EMBEDDING_FILE=path+'/input/crawl-300d-2M.vec'\n",
    "# TRAIN_DATA_FILE=path+'/input/train.csv'\n",
    "# TEST_DATA_FILE=path+'/input/test.csv'\n",
    "\n",
    "EMBEDDING_FILE='/public/models/glove/glove.840B.300d.txt'\n",
    "TRAIN_DATA_FILE='/public/toxic_comments/train.csv'\n",
    "TEST_DATA_FILE='/public/toxic_comments/test.csv'\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "MAX_NB_WORDS = 100000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "num_lstm = 300\n",
    "num_dense = 256\n",
    "rate_drop_lstm = 0.2\n",
    "rate_drop_dense = 0.2\n",
    "\n",
    "act = 'relu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanData(text, stemming = False, lemmatize=False):    \n",
    "    text = text.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+\\-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    if stemming:\n",
    "        st = PorterStemmer()\n",
    "        txt = \" \".join([st.stem(w) for w in text.split()])\n",
    "    if lemmatize:\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        txt = \" \".join([wordnet_lemmatizer.lemmatize(w) for w in text.split()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors\n",
      "Found 2195895 word vectors of glove.\n",
      "-0.01444638 0.47249147\n",
      "Total 2195895 word vectors.\n",
      "Processing text dataset\n",
      "Found 292462 unique tokens\n",
      "Shape of data tensor: (159571, 200)\n",
      "Shape of label tensor: (159571, 6)\n",
      "Shape of test_data tensor: (153164, 200)\n"
     ]
    }
   ],
   "source": [
    "print('Indexing word vectors')\n",
    "\n",
    "count = 0\n",
    "embeddings_index = {}\n",
    "f = open(EMBEDDING_FILE)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = ' '.join(values[:-300])\n",
    "    coefs = np.asarray(values[-300:], dtype='float32')\n",
    "    embeddings_index[word] = coefs.reshape(-1)\n",
    "    coef = embeddings_index[word]\n",
    "f.close()\n",
    "\n",
    "print('Found %d word vectors of glove.' % len(embeddings_index))\n",
    "emb_mean,emb_std = coef.mean(), coef.std()\n",
    "print(emb_mean,emb_std)\n",
    "\n",
    "print('Total %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test_df = pd.read_csv(TEST_DATA_FILE)\n",
    "\n",
    "print('Processing text dataset')\n",
    "\n",
    "train_df['comment_text'] = train_df['comment_text'].map(lambda x: cleanData(x,  stemming = False, \n",
    "                                                                            lemmatize=False))\n",
    "test_df['comment_text'] = test_df['comment_text'].map(lambda x: cleanData(x,  stemming = False, \n",
    "                                                                          lemmatize=False))\n",
    "\n",
    "#Regex to remove all Non-Alpha Numeric and space\n",
    "special_character_removal=re.compile(r'[^a-z\\d ]',re.IGNORECASE)\n",
    "#regex to replace all numerics\n",
    "replace_numbers=re.compile(r'\\d+',re.IGNORECASE)\n",
    "\n",
    "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    #Remove Special Characters\n",
    "    text=special_character_removal.sub('',text)\n",
    "    #Replace Numbers\n",
    "    text=replace_numbers.sub('n',text)\n",
    "\n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    return(text)\n",
    "\n",
    "\n",
    "list_sentences_train = train_df[\"comment_text\"].fillna(\"NA\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train_df[list_classes].values\n",
    "list_sentences_test = test_df[\"comment_text\"].fillna(\"NA\").values\n",
    "\n",
    "\n",
    "comments = []\n",
    "for text in list_sentences_train:\n",
    "    comments.append(text_to_wordlist(text))\n",
    "    \n",
    "test_comments=[]\n",
    "for text in list_sentences_test:\n",
    "    test_comments.append(text_to_wordlist(text))\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(comments + test_comments)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(comments)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_comments)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', y.shape)\n",
    "\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of test_data tensor:', test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (159571, 200)\n",
      "Shape of label tensor: (159571, 6)\n",
      "Shape of test_data tensor: (153164, 200)\n"
     ]
    }
   ],
   "source": [
    "data_post = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH,padding='post', truncating='post')\n",
    "print('Shape of data tensor:', data_post.shape)\n",
    "print('Shape of label tensor:', y.shape)\n",
    "\n",
    "test_data_post = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "print('Shape of test_data tensor:', test_data_post.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: 21603\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix')\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from os.path import dirname\n",
    "from keras import initializers\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "class AttentionWeightedAverage(Layer):\n",
    "    \"\"\"\n",
    "    Computes a weighted average of the different channels across timesteps.\n",
    "    Uses 1 parameter pr. channel to compute the attention value for a single timestep.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, return_attention=False, **kwargs):\n",
    "        self.init = initializers.get('uniform')\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        super(AttentionWeightedAverage, self).__init__(** kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(ndim=3)]\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[2], 1),\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 initializer=self.init)\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttentionWeightedAverage, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # computes a probability distribution over the timesteps\n",
    "        # uses 'max trick' for numerical stability\n",
    "        # reshape is done to avoid issue with Tensorflow\n",
    "        # and 1-dimensional weights\n",
    "        logits = K.dot(x, self.W)\n",
    "        x_shape = K.shape(x)\n",
    "        logits = K.reshape(logits, (x_shape[0], x_shape[1]))\n",
    "        ai = K.exp(logits - K.max(logits, axis=-1, keepdims=True))\n",
    "\n",
    "        # masked timesteps have zero weight\n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            ai = ai * mask\n",
    "        att_weights = ai / (K.sum(ai, axis=1, keepdims=True) + K.epsilon())\n",
    "        weighted_input = x * K.expand_dims(att_weights)\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "        if self.return_attention:\n",
    "            return [result, att_weights]\n",
    "        return result\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return self.compute_output_shape(input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_len = input_shape[2]\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]\n",
    "        return (input_shape[0], output_len)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        if isinstance(input_mask, list):\n",
    "            return [None] * len(input_mask)\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Nadam\n",
    "filter_sizes = [1,2,3,5]\n",
    "num_filters = 32\n",
    "\n",
    "def deepmoji_architecture(nb_classes, maxlen, feature_output=False, embed_dropout_rate=0, final_dropout_rate=0, embed_l2=1E-6, return_attention=False):\n",
    "    # define embedding layer that turns word tokens into vectors\n",
    "    # an activation function is used to bound the values of the embedding\n",
    "    model_input = Input(shape=(maxlen,), dtype='int32')\n",
    "    model_input_post = Input(shape=(maxlen,), dtype='int32')\n",
    "    \n",
    "    embed_reg = L1L2(l2=embed_l2) if embed_l2 != 0 else None\n",
    "    embed = Embedding(nb_words,\n",
    "        EMBEDDING_DIM,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        embeddings_regularizer=embed_reg,\n",
    "        trainable=False)\n",
    "    x1 = embed(model_input)\n",
    "    x1 = SpatialDropout1D(0.4)(x1)\n",
    "    x1 = Reshape((200, 300, 1))(x1)\n",
    "    \n",
    "    pooled_pre = []\n",
    "    for i in filter_sizes:\n",
    "    \n",
    "        conv_pre = Conv2D(num_filters, kernel_size=(i, 300), kernel_initializer='normal',\n",
    "                    activation='elu')(x1)\n",
    "    \n",
    "        maxpool_pre = MaxPool2D(pool_size=(maxlen - i + 1, 1))(conv_pre)\n",
    "        avepool_pre = AveragePooling2D(pool_size=(maxlen - i + 1, 1))(conv_pre)\n",
    "        globalmax_pre = GlobalMaxPooling2D()(conv_pre)\n",
    "        pooled_pre.append(globalmax_pre)\n",
    "        \n",
    "    z1 = Concatenate(axis=1)(pooled_pre)   \n",
    "    z1 = Dropout(0.2)(z1)\n",
    "    \n",
    "    embed_post = Embedding(nb_words,\n",
    "        EMBEDDING_DIM,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        embeddings_regularizer=embed_reg,\n",
    "        trainable=False)\n",
    "    x1_post = embed(model_input_post)\n",
    "    x1_post = SpatialDropout1D(0.4)(x1_post)\n",
    "    x1_post = Reshape((200, 300, 1))(x1_post)\n",
    "    \n",
    "    pooled_post = []\n",
    "    for i in filter_sizes:\n",
    "\n",
    "        conv_post = Conv2D(num_filters, kernel_size=(i, 300), kernel_initializer='normal',\n",
    "                    activation='elu')(x1_post)\n",
    "    \n",
    "        maxpool_post = MaxPool2D(pool_size=(maxlen - i + 1, 1))(conv_post)\n",
    "        avepool_post = AveragePooling2D(pool_size=(maxlen - i + 1, 1))(conv_post)\n",
    "        globalmax_post = GlobalMaxPooling2D()(conv_post)\n",
    "        pooled_post.append(globalmax_post)\n",
    "        \n",
    "    z1_post = Concatenate(axis=1)(pooled_post)   \n",
    "    z1_post = Dropout(0.2)(z1_post)\n",
    "    \n",
    "    z = concatenate([z1,z1_post])\n",
    "    outputs = Dense(6, activation=\"sigmoid\")(z)\n",
    "    model = Model(inputs=[model_input,model_input_post], outputs=outputs, name=\"Textcnn2D\")\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "        optimizer=Adam(lr=1e-3,decay=0),\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "import numpy as np\n",
    "\n",
    "test_predicts_list = []\n",
    "\n",
    "def train_folds(data,data_post, y, fold_count, batch_size):\n",
    "    print(\"Starting to train models...\")\n",
    "    fold_size = len(data) // fold_count\n",
    "    models = []\n",
    "    for fold_id in range(0, fold_count):\n",
    "        fold_start = fold_size * fold_id\n",
    "        fold_end = fold_start + fold_size\n",
    "\n",
    "        if fold_id == fold_size - 1:\n",
    "            fold_end = len(data)\n",
    "\n",
    "        print(\"Fold {0}\".format(fold_id))\n",
    "        \n",
    "        train_x = np.concatenate([data[:fold_start], data[fold_end:]])\n",
    "        train_xp = np.concatenate([data_post[:fold_start], data_post[fold_end:]])\n",
    "        train_y = np.concatenate([y[:fold_start], y[fold_end:]])\n",
    "\n",
    "        val_x = data[fold_start:fold_end]\n",
    "        val_xp = data_post[fold_start:fold_end]\n",
    "        val_y = y[fold_start:fold_end]\n",
    "        \n",
    "        file_path=\"textcnn2D_fold{0}.h5\".format(fold_id)\n",
    "#         model = get_model()\n",
    "        model = deepmoji_architecture(6, MAX_SEQUENCE_LENGTH, feature_output=False, embed_dropout_rate=0.4, final_dropout_rate=0.4, embed_l2=0, \n",
    "                              return_attention=False)\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3)\n",
    "        callbacks_list = [checkpoint, early] \n",
    "\n",
    "        hist = model.fit([train_x, train_xp], train_y, epochs=15, batch_size=128, shuffle=True, \n",
    "                         validation_data=([val_x, val_xp], val_y), callbacks = callbacks_list, verbose=1)\n",
    "        model.load_weights(file_path)\n",
    "        best_score = min(hist.history['val_loss'])\n",
    "        \n",
    "        print(\"Fold {0} loss {1}\".format(fold_id, best_score))\n",
    "        print(\"Predicting validation...\")\n",
    "        val_predicts_path = \"textcnn2D_val_predicts{0}.npy\".format(fold_id)\n",
    "        val_predicts = model.predict([val_x, val_xp], batch_size=1024, verbose=1)\n",
    "        np.save(val_predicts_path, val_predicts)\n",
    "        \n",
    "        print(\"Predicting results...\")\n",
    "        test_predicts_path = \"textcnn2D_test_predicts{0}.npy\".format(fold_id)\n",
    "        test_predicts = model.predict([test_data, test_data_post], batch_size=1024, verbose=1)\n",
    "        test_predicts_list.append(test_predicts)\n",
    "        np.save(test_predicts_path, test_predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train models...\n",
      "Fold 0\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/15\n",
      "143614/143614 [==============================] - 54s 374us/step - loss: 0.0649 - acc: 0.9781 - val_loss: 0.0509 - val_acc: 0.9810\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05094, saving model to textcnn2D_fold0.h5\n",
      "Epoch 2/15\n",
      "143614/143614 [==============================] - 51s 354us/step - loss: 0.0492 - acc: 0.9820 - val_loss: 0.0490 - val_acc: 0.9817\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05094 to 0.04899, saving model to textcnn2D_fold0.h5\n",
      "Epoch 3/15\n",
      "143614/143614 [==============================] - 51s 355us/step - loss: 0.0464 - acc: 0.9826 - val_loss: 0.0429 - val_acc: 0.9836\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04899 to 0.04295, saving model to textcnn2D_fold0.h5\n",
      "Epoch 4/15\n",
      "143614/143614 [==============================] - 51s 356us/step - loss: 0.0445 - acc: 0.9832 - val_loss: 0.0441 - val_acc: 0.9830\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/15\n",
      "143614/143614 [==============================] - 51s 355us/step - loss: 0.0432 - acc: 0.9837 - val_loss: 0.0450 - val_acc: 0.9829\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/15\n",
      "143614/143614 [==============================] - 51s 354us/step - loss: 0.0421 - acc: 0.9841 - val_loss: 0.0431 - val_acc: 0.9836\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Fold 0 loss 0.04294932899617152\n",
      "Predicting validation...\n",
      "15957/15957 [==============================] - 4s 242us/step\n",
      "Predicting results...\n",
      "153164/153164 [==============================] - 21s 138us/step\n",
      "Fold 1\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/15\n",
      "143614/143614 [==============================] - 51s 357us/step - loss: 0.0622 - acc: 0.9789 - val_loss: 0.0503 - val_acc: 0.9812\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05025, saving model to textcnn2D_fold1.h5\n",
      "Epoch 2/15\n",
      "143614/143614 [==============================] - 51s 354us/step - loss: 0.0490 - acc: 0.9820 - val_loss: 0.0479 - val_acc: 0.9818\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05025 to 0.04788, saving model to textcnn2D_fold1.h5\n",
      "Epoch 3/15\n",
      "143614/143614 [==============================] - 51s 354us/step - loss: 0.0459 - acc: 0.9828 - val_loss: 0.0443 - val_acc: 0.9830\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04788 to 0.04434, saving model to textcnn2D_fold1.h5\n",
      "Epoch 4/15\n",
      "143614/143614 [==============================] - 51s 356us/step - loss: 0.0444 - acc: 0.9832 - val_loss: 0.0445 - val_acc: 0.9831\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/15\n",
      "143614/143614 [==============================] - 51s 355us/step - loss: 0.0427 - acc: 0.9837 - val_loss: 0.0435 - val_acc: 0.9832\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.04434 to 0.04351, saving model to textcnn2D_fold1.h5\n",
      "Epoch 6/15\n",
      "143614/143614 [==============================] - 51s 356us/step - loss: 0.0421 - acc: 0.9841 - val_loss: 0.0462 - val_acc: 0.9823\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/15\n",
      "143614/143614 [==============================] - 51s 353us/step - loss: 0.0408 - acc: 0.9843 - val_loss: 0.0429 - val_acc: 0.9835\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.04351 to 0.04291, saving model to textcnn2D_fold1.h5\n",
      "Epoch 8/15\n",
      "143614/143614 [==============================] - 51s 354us/step - loss: 0.0401 - acc: 0.9846 - val_loss: 0.0431 - val_acc: 0.9836\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/15\n",
      "143614/143614 [==============================] - 51s 353us/step - loss: 0.0395 - acc: 0.9849 - val_loss: 0.0439 - val_acc: 0.9831\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/15\n",
      "143614/143614 [==============================] - 51s 355us/step - loss: 0.0390 - acc: 0.9851 - val_loss: 0.0455 - val_acc: 0.9829\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Fold 1 loss 0.042909311338303835\n",
      "Predicting validation...\n",
      "15957/15957 [==============================] - 2s 141us/step\n",
      "Predicting results...\n",
      "153164/153164 [==============================] - 21s 134us/step\n",
      "Fold 2\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/15\n",
      "143614/143614 [==============================] - 52s 362us/step - loss: 0.0640 - acc: 0.9783 - val_loss: 0.0464 - val_acc: 0.9824\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04643, saving model to textcnn2D_fold2.h5\n",
      "Epoch 2/15\n",
      "143614/143614 [==============================] - 51s 356us/step - loss: 0.0492 - acc: 0.9819 - val_loss: 0.0431 - val_acc: 0.9834\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04643 to 0.04307, saving model to textcnn2D_fold2.h5\n",
      "Epoch 3/15\n",
      "143614/143614 [==============================] - 51s 355us/step - loss: 0.0461 - acc: 0.9827 - val_loss: 0.0455 - val_acc: 0.9827\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/15\n",
      "143614/143614 [==============================] - 51s 353us/step - loss: 0.0449 - acc: 0.9832 - val_loss: 0.0441 - val_acc: 0.9831\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/15\n",
      "143614/143614 [==============================] - 51s 355us/step - loss: 0.0435 - acc: 0.9835 - val_loss: 0.0432 - val_acc: 0.9833\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Fold 2 loss 0.04307470413446314\n",
      "Predicting validation...\n",
      "15957/15957 [==============================] - 2s 142us/step\n",
      "Predicting results...\n",
      "153164/153164 [==============================] - 21s 134us/step\n",
      "Fold 3\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/15\n",
      "143614/143614 [==============================] - 51s 358us/step - loss: 0.0670 - acc: 0.9773 - val_loss: 0.0534 - val_acc: 0.9809\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05344, saving model to textcnn2D_fold3.h5\n",
      "Epoch 2/15\n",
      "143614/143614 [==============================] - 51s 353us/step - loss: 0.0497 - acc: 0.9818 - val_loss: 0.0460 - val_acc: 0.9825\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05344 to 0.04598, saving model to textcnn2D_fold3.h5\n",
      "Epoch 3/15\n",
      "143614/143614 [==============================] - 51s 354us/step - loss: 0.0465 - acc: 0.9827 - val_loss: 0.0443 - val_acc: 0.9832\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04598 to 0.04430, saving model to textcnn2D_fold3.h5\n",
      "Epoch 4/15\n",
      "143614/143614 [==============================] - 51s 353us/step - loss: 0.0444 - acc: 0.9832 - val_loss: 0.0438 - val_acc: 0.9835\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04430 to 0.04383, saving model to textcnn2D_fold3.h5\n",
      "Epoch 5/15\n",
      "143614/143614 [==============================] - 51s 353us/step - loss: 0.0431 - acc: 0.9837 - val_loss: 0.0460 - val_acc: 0.9828\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/15\n",
      "143614/143614 [==============================] - 50s 351us/step - loss: 0.0420 - acc: 0.9841 - val_loss: 0.0454 - val_acc: 0.9833\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/15\n",
      "143614/143614 [==============================] - 51s 352us/step - loss: 0.0411 - acc: 0.9843 - val_loss: 0.0435 - val_acc: 0.9838\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.04383 to 0.04353, saving model to textcnn2D_fold3.h5\n",
      "Epoch 8/15\n",
      "143614/143614 [==============================] - 51s 352us/step - loss: 0.0404 - acc: 0.9847 - val_loss: 0.0436 - val_acc: 0.9836\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/15\n",
      "143614/143614 [==============================] - 51s 354us/step - loss: 0.0394 - acc: 0.9849 - val_loss: 0.0444 - val_acc: 0.9831\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/15\n",
      "143614/143614 [==============================] - 51s 353us/step - loss: 0.0392 - acc: 0.9850 - val_loss: 0.0440 - val_acc: 0.9835\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Fold 3 loss 0.043525987005162046\n",
      "Predicting validation...\n",
      "15957/15957 [==============================] - 2s 147us/step\n",
      "Predicting results...\n",
      "153164/153164 [==============================] - 21s 134us/step\n",
      "Fold 4\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/15\n",
      "143614/143614 [==============================] - 52s 364us/step - loss: 0.0702 - acc: 0.9763 - val_loss: 0.0591 - val_acc: 0.9793\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05911, saving model to textcnn2D_fold4.h5\n",
      "Epoch 2/15\n",
      "143614/143614 [==============================] - 51s 355us/step - loss: 0.0497 - acc: 0.9817 - val_loss: 0.0478 - val_acc: 0.9822\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05911 to 0.04784, saving model to textcnn2D_fold4.h5\n",
      "Epoch 3/15\n",
      "143614/143614 [==============================] - 51s 355us/step - loss: 0.0470 - acc: 0.9826 - val_loss: 0.0445 - val_acc: 0.9832\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04784 to 0.04452, saving model to textcnn2D_fold4.h5\n",
      "Epoch 4/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143614/143614 [==============================] - 51s 354us/step - loss: 0.0450 - acc: 0.9830 - val_loss: 0.0431 - val_acc: 0.9836\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04452 to 0.04313, saving model to textcnn2D_fold4.h5\n",
      "Epoch 5/15\n",
      "143614/143614 [==============================] - 51s 354us/step - loss: 0.0435 - acc: 0.9835 - val_loss: 0.0426 - val_acc: 0.9837\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.04313 to 0.04258, saving model to textcnn2D_fold4.h5\n",
      "Epoch 6/15\n",
      "143614/143614 [==============================] - 51s 356us/step - loss: 0.0426 - acc: 0.9839 - val_loss: 0.0432 - val_acc: 0.9837\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/15\n",
      "143614/143614 [==============================] - 51s 357us/step - loss: 0.0414 - acc: 0.9842 - val_loss: 0.0428 - val_acc: 0.9839\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/15\n",
      "143614/143614 [==============================] - 51s 357us/step - loss: 0.0406 - acc: 0.9845 - val_loss: 0.0443 - val_acc: 0.9834\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Fold 4 loss 0.04258317753421399\n",
      "Predicting validation...\n",
      "15957/15957 [==============================] - 2s 150us/step\n",
      "Predicting results...\n",
      "153164/153164 [==============================] - 21s 134us/step\n",
      "Fold 5\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/15\n",
      "143614/143614 [==============================] - 52s 362us/step - loss: 0.0693 - acc: 0.9764 - val_loss: 0.0459 - val_acc: 0.9832\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04585, saving model to textcnn2D_fold5.h5\n",
      "Epoch 2/15\n",
      "143614/143614 [==============================] - 51s 355us/step - loss: 0.0501 - acc: 0.9816 - val_loss: 0.0445 - val_acc: 0.9834\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04585 to 0.04453, saving model to textcnn2D_fold5.h5\n",
      "Epoch 3/15\n",
      "143614/143614 [==============================] - 51s 356us/step - loss: 0.0470 - acc: 0.9826 - val_loss: 0.0417 - val_acc: 0.9842\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04453 to 0.04168, saving model to textcnn2D_fold5.h5\n",
      "Epoch 4/15\n",
      "143614/143614 [==============================] - 51s 357us/step - loss: 0.0450 - acc: 0.9830 - val_loss: 0.0415 - val_acc: 0.9841\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04168 to 0.04146, saving model to textcnn2D_fold5.h5\n",
      "Epoch 5/15\n",
      "143614/143614 [==============================] - 51s 357us/step - loss: 0.0434 - acc: 0.9836 - val_loss: 0.0421 - val_acc: 0.9841\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/15\n",
      "143614/143614 [==============================] - 51s 356us/step - loss: 0.0426 - acc: 0.9837 - val_loss: 0.0410 - val_acc: 0.9843\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.04146 to 0.04101, saving model to textcnn2D_fold5.h5\n",
      "Epoch 7/15\n",
      "143614/143614 [==============================] - 51s 356us/step - loss: 0.0418 - acc: 0.9842 - val_loss: 0.0418 - val_acc: 0.9841\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/15\n",
      "143614/143614 [==============================] - 51s 355us/step - loss: 0.0404 - acc: 0.9846 - val_loss: 0.0419 - val_acc: 0.9839\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/15\n",
      "143614/143614 [==============================] - 51s 355us/step - loss: 0.0400 - acc: 0.9847 - val_loss: 0.0421 - val_acc: 0.9841\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Fold 5 loss 0.04101396713632535\n",
      "Predicting validation...\n",
      "15957/15957 [==============================] - 2s 150us/step\n",
      "Predicting results...\n",
      "153164/153164 [==============================] - 21s 134us/step\n",
      "Fold 6\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/15\n",
      "143614/143614 [==============================] - 52s 362us/step - loss: 0.0660 - acc: 0.9778 - val_loss: 0.0515 - val_acc: 0.9810\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05152, saving model to textcnn2D_fold6.h5\n",
      "Epoch 2/15\n",
      "143614/143614 [==============================] - 51s 356us/step - loss: 0.0492 - acc: 0.9819 - val_loss: 0.0459 - val_acc: 0.9828\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05152 to 0.04589, saving model to textcnn2D_fold6.h5\n",
      "Epoch 3/15\n",
      "143614/143614 [==============================] - 51s 354us/step - loss: 0.0465 - acc: 0.9826 - val_loss: 0.0460 - val_acc: 0.9829\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/15\n",
      "143614/143614 [==============================] - 51s 355us/step - loss: 0.0445 - acc: 0.9832 - val_loss: 0.0455 - val_acc: 0.9831\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04589 to 0.04547, saving model to textcnn2D_fold6.h5\n",
      "Epoch 5/15\n",
      "143614/143614 [==============================] - 51s 354us/step - loss: 0.0435 - acc: 0.9835 - val_loss: 0.0451 - val_acc: 0.9830\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.04547 to 0.04508, saving model to textcnn2D_fold6.h5\n",
      "Epoch 6/15\n",
      "143614/143614 [==============================] - 51s 354us/step - loss: 0.0421 - acc: 0.9840 - val_loss: 0.0416 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.04508 to 0.04155, saving model to textcnn2D_fold6.h5\n",
      "Epoch 7/15\n",
      "143614/143614 [==============================] - 51s 354us/step - loss: 0.0412 - acc: 0.9843 - val_loss: 0.0416 - val_acc: 0.9841\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/15\n",
      "143614/143614 [==============================] - 51s 354us/step - loss: 0.0406 - acc: 0.9845 - val_loss: 0.0418 - val_acc: 0.9842\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/15\n",
      "143614/143614 [==============================] - 51s 355us/step - loss: 0.0398 - acc: 0.9847 - val_loss: 0.0441 - val_acc: 0.9832\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Fold 6 loss 0.04155166598805313\n",
      "Predicting validation...\n",
      "15957/15957 [==============================] - 3s 157us/step\n",
      "Predicting results...\n",
      "153164/153164 [==============================] - 21s 134us/step\n",
      "Fold 7\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/15\n",
      "143614/143614 [==============================] - 53s 367us/step - loss: 0.0661 - acc: 0.9777 - val_loss: 0.0491 - val_acc: 0.9817\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04910, saving model to textcnn2D_fold7.h5\n",
      "Epoch 2/15\n",
      "143614/143614 [==============================] - 51s 357us/step - loss: 0.0495 - acc: 0.9818 - val_loss: 0.0458 - val_acc: 0.9823\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04910 to 0.04581, saving model to textcnn2D_fold7.h5\n",
      "Epoch 3/15\n",
      "143614/143614 [==============================] - 51s 357us/step - loss: 0.0462 - acc: 0.9827 - val_loss: 0.0470 - val_acc: 0.9822\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/15\n",
      "143614/143614 [==============================] - 51s 357us/step - loss: 0.0446 - acc: 0.9832 - val_loss: 0.0439 - val_acc: 0.9830\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04581 to 0.04388, saving model to textcnn2D_fold7.h5\n",
      "Epoch 5/15\n",
      "143614/143614 [==============================] - 51s 356us/step - loss: 0.0430 - acc: 0.9838 - val_loss: 0.0429 - val_acc: 0.9832\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.04388 to 0.04286, saving model to textcnn2D_fold7.h5\n",
      "Epoch 6/15\n",
      "143614/143614 [==============================] - 51s 357us/step - loss: 0.0418 - acc: 0.9840 - val_loss: 0.0440 - val_acc: 0.9833\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/15\n",
      "143614/143614 [==============================] - 51s 357us/step - loss: 0.0411 - acc: 0.9843 - val_loss: 0.0427 - val_acc: 0.9835\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.04286 to 0.04273, saving model to textcnn2D_fold7.h5\n",
      "Epoch 8/15\n",
      "143614/143614 [==============================] - 51s 358us/step - loss: 0.0404 - acc: 0.9845 - val_loss: 0.0425 - val_acc: 0.9836\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.04273 to 0.04246, saving model to textcnn2D_fold7.h5\n",
      "Epoch 9/15\n",
      "143614/143614 [==============================] - 51s 358us/step - loss: 0.0394 - acc: 0.9849 - val_loss: 0.0426 - val_acc: 0.9837\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/15\n",
      "143614/143614 [==============================] - 51s 356us/step - loss: 0.0389 - acc: 0.9852 - val_loss: 0.0427 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/15\n",
      "143614/143614 [==============================] - 51s 357us/step - loss: 0.0384 - acc: 0.9853 - val_loss: 0.0438 - val_acc: 0.9833\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Fold 7 loss 0.04245513321153276\n",
      "Predicting validation...\n",
      "15957/15957 [==============================] - 3s 158us/step\n",
      "Predicting results...\n",
      "153164/153164 [==============================] - 21s 134us/step\n",
      "Fold 8\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/15\n",
      "143614/143614 [==============================] - 54s 373us/step - loss: 0.0697 - acc: 0.9763 - val_loss: 0.0504 - val_acc: 0.9815\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05037, saving model to textcnn2D_fold8.h5\n",
      "Epoch 2/15\n",
      "143614/143614 [==============================] - 51s 357us/step - loss: 0.0502 - acc: 0.9817 - val_loss: 0.0432 - val_acc: 0.9835\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05037 to 0.04317, saving model to textcnn2D_fold8.h5\n",
      "Epoch 3/15\n",
      "143614/143614 [==============================] - 51s 358us/step - loss: 0.0469 - acc: 0.9825 - val_loss: 0.0438 - val_acc: 0.9832\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/15\n",
      "143614/143614 [==============================] - 51s 358us/step - loss: 0.0452 - acc: 0.9829 - val_loss: 0.0431 - val_acc: 0.9834\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04317 to 0.04308, saving model to textcnn2D_fold8.h5\n",
      "Epoch 5/15\n",
      "143614/143614 [==============================] - 52s 359us/step - loss: 0.0439 - acc: 0.9835 - val_loss: 0.0415 - val_acc: 0.9842\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.04308 to 0.04149, saving model to textcnn2D_fold8.h5\n",
      "Epoch 6/15\n",
      "143614/143614 [==============================] - 51s 357us/step - loss: 0.0427 - acc: 0.9838 - val_loss: 0.0421 - val_acc: 0.9839\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/15\n",
      "143614/143614 [==============================] - 51s 357us/step - loss: 0.0416 - acc: 0.9841 - val_loss: 0.0413 - val_acc: 0.9843\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.04149 to 0.04125, saving model to textcnn2D_fold8.h5\n",
      "Epoch 8/15\n",
      "143614/143614 [==============================] - 51s 356us/step - loss: 0.0411 - acc: 0.9841 - val_loss: 0.0413 - val_acc: 0.9844\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/15\n",
      "143614/143614 [==============================] - 51s 356us/step - loss: 0.0398 - acc: 0.9847 - val_loss: 0.0413 - val_acc: 0.9845\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/15\n",
      "143614/143614 [==============================] - 52s 359us/step - loss: 0.0394 - acc: 0.9849 - val_loss: 0.0420 - val_acc: 0.9842\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Fold 8 loss 0.041250980434232004\n",
      "Predicting validation...\n",
      "15957/15957 [==============================] - 3s 160us/step\n",
      "Predicting results...\n",
      "153164/153164 [==============================] - 21s 134us/step\n",
      "Fold 9\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/15\n",
      "143614/143614 [==============================] - 53s 369us/step - loss: 0.0661 - acc: 0.9775 - val_loss: 0.0486 - val_acc: 0.9816\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04860, saving model to textcnn2D_fold9.h5\n",
      "Epoch 2/15\n",
      "143614/143614 [==============================] - 51s 358us/step - loss: 0.0492 - acc: 0.9818 - val_loss: 0.0506 - val_acc: 0.9810\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/15\n",
      "143614/143614 [==============================] - 51s 357us/step - loss: 0.0461 - acc: 0.9827 - val_loss: 0.0464 - val_acc: 0.9822\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04860 to 0.04643, saving model to textcnn2D_fold9.h5\n",
      "Epoch 4/15\n",
      "143614/143614 [==============================] - 51s 357us/step - loss: 0.0447 - acc: 0.9831 - val_loss: 0.0474 - val_acc: 0.9823\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/15\n",
      "143614/143614 [==============================] - 51s 358us/step - loss: 0.0433 - acc: 0.9837 - val_loss: 0.0446 - val_acc: 0.9832\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.04643 to 0.04458, saving model to textcnn2D_fold9.h5\n",
      "Epoch 6/15\n",
      "143614/143614 [==============================] - 51s 358us/step - loss: 0.0419 - acc: 0.9841 - val_loss: 0.0440 - val_acc: 0.9830\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.04458 to 0.04395, saving model to textcnn2D_fold9.h5\n",
      "Epoch 7/15\n",
      "143614/143614 [==============================] - 52s 361us/step - loss: 0.0412 - acc: 0.9844 - val_loss: 0.0439 - val_acc: 0.9831\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.04395 to 0.04393, saving model to textcnn2D_fold9.h5\n",
      "Epoch 8/15\n",
      "143614/143614 [==============================] - 52s 360us/step - loss: 0.0403 - acc: 0.9846 - val_loss: 0.0438 - val_acc: 0.9829\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.04393 to 0.04385, saving model to textcnn2D_fold9.h5\n",
      "Epoch 9/15\n",
      "143614/143614 [==============================] - 52s 359us/step - loss: 0.0396 - acc: 0.9848 - val_loss: 0.0435 - val_acc: 0.9836\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.04385 to 0.04353, saving model to textcnn2D_fold9.h5\n",
      "Epoch 10/15\n",
      "143614/143614 [==============================] - 51s 359us/step - loss: 0.0387 - acc: 0.9851 - val_loss: 0.0435 - val_acc: 0.9834\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.04353 to 0.04351, saving model to textcnn2D_fold9.h5\n",
      "Epoch 11/15\n",
      "143614/143614 [==============================] - 51s 358us/step - loss: 0.0383 - acc: 0.9853 - val_loss: 0.0436 - val_acc: 0.9832\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/15\n",
      "143614/143614 [==============================] - 51s 357us/step - loss: 0.0379 - acc: 0.9854 - val_loss: 0.0440 - val_acc: 0.9835\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/15\n",
      "143614/143614 [==============================] - 51s 357us/step - loss: 0.0374 - acc: 0.9856 - val_loss: 0.0447 - val_acc: 0.9832\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Fold 9 loss 0.04351138351408676\n",
      "Predicting validation...\n",
      "15957/15957 [==============================] - 3s 169us/step\n",
      "Predicting results...\n",
      "153164/153164 [==============================] - 21s 135us/step\n"
     ]
    }
   ],
   "source": [
    "train_folds(data, data_post, y, 10, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "CLASSES = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "print(len(test_predicts_list))\n",
    "test_predicts_am = np.zeros(test_predicts_list[0].shape)\n",
    "\n",
    "for fold_predict in test_predicts_list:\n",
    "    test_predicts_am += fold_predict\n",
    "\n",
    "test_predicts_am = (test_predicts_am / len(test_predicts_list))\n",
    "\n",
    "test_ids = test_df[\"id\"].values\n",
    "test_ids = test_ids.reshape((len(test_ids), 1))\n",
    "\n",
    "test_predicts_am = pd.DataFrame(data=test_predicts_am, columns=CLASSES)\n",
    "test_predicts_am[\"id\"] = test_ids\n",
    "test_predicts_am = test_predicts_am[[\"id\"] + CLASSES]\n",
    "test_predicts_am.to_csv(\"10fold_attngru_am.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicts = np.ones(test_predicts_list[0].shape)\n",
    "\n",
    "for fold_predict in test_predicts_list:\n",
    "    test_predicts *= fold_predict\n",
    "\n",
    "test_predicts **= (1. / len(test_predicts_list))\n",
    "\n",
    "test_ids = test_df[\"id\"].values\n",
    "test_ids = test_ids.reshape((len(test_ids), 1))\n",
    "\n",
    "test_predicts = pd.DataFrame(data=test_predicts, columns=CLASSES)\n",
    "test_predicts[\"id\"] = test_ids\n",
    "test_predicts = test_predicts[[\"id\"] + CLASSES]\n",
    "test_predicts.to_csv(\"10fold_attngru_gm.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
